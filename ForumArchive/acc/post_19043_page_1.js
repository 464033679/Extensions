[{"Owner":"Polpatch","Date":"2015-12-03T21:34:49Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_div_gt_Hello everyone!_lt_/div_gt__lt_div_gt_I plan to use the Kinect v2 (that for xboxOne) to move an avatar in my scene._lt_/div_gt__lt_div_gt_My idea was to gain the quaternions of jointOrientations and then change the appropriate bone matrix._lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Nevertheless_co_ the jointOrientations are expressed in global rotations (each jointOrientation indicates the direction of the joint in absolute coordinates) while_co_ if I understand it_co_ I can only modify the local matrix of skeleton bone._lt_br_gt__lt_br_gt_So I am trying to convert local jointOrientation in local rotation_dd__lt_/div_gt__lt_div_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_var joint_sm_     //the joint of kinectvar parent _eq_ joint.Parent()_sm_var localOrientation _eq_ BABYLON.Quaternion.Inverse(parent.Orientation).Multiply(joint.Orientation)_sm__lt_/pre_gt__lt_/div_gt__lt_p_gt_But I_t_m having trouble in the transformation of the reference coordinate between kinect joints and avatar bones int the babylon scene..._lt_/p_gt__lt_p_gt_I tried to change the axes by swapping values (x_co_ y_co_ z)_co_ but I_t_m probably wrong_lt_/p_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_var kinectOrientation_sm_        //orientation of the joint expressed in quaternionreturn new BABYLON.Quaternion(kinectOrientation.y_co_                              kinectOrientation.x_co_                              kinectOrientation.z_co_                              kinectOrientation.w)_sm_   //this is just one example_co_ I have tried in different cases_lt_/pre_gt__lt_p_gt_Do you have any advice?_lt_/p_gt__lt_p_gt_Thanks in advance_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Deltakosh","Date":"2015-12-03T23:47:35Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Is Kinect using a left or right handed system?_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Babylon.js is left handed for the record_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_if Kinect is right handed you should do_dd__lt_/p_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_return new BABYLON.Quaternion(kinectOrientation.x_co_                              kinectOrientation.z_co_                              kinectOrientation.y_co_                              kinectOrientation.w)_sm_  _lt_/pre_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Polpatch","Date":"2015-12-04T12:25:05Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_div_gt_Kinect documentation is very poor_co_ but looking around I found that the Kinect uses a right-handed coordinate system._lt_/div_gt__lt_div_gt_I tried to use your combination on a single bone (right forearm) of the avatar but I have not had a correct reply._lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_I retried once again with this new combination and the forearm moves correctly ...._lt_/div_gt__lt_div_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_var orient _eq_ new BABYLON.Quaternion(kinectOrientation.x_co_                                    -kinectOrientation.y_co_                                    -kinectOrientation.z_co_                                     kinectOrientation.w)_sm__lt_/pre_gt__lt_/div_gt__lt_div_gt__lt_div_gt_Perhaps this could be given by the different local reference axes of each local bone respect the scene._lt_/div_gt__lt_/div_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_However_co_ your answer made me realize that my reasoning was correct. _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_biggrin.png_qt_ alt_eq__qt__dd_D_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/biggrin@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-12-04T16:29:11Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_I am struggling this as well trying to read motion capture files directly.  The definition I see all over the place for left verses right handed_co_ are about which direct is the positive direction of the Z axis _lt_a href_eq__qt_https_dd_//www.evl.uic.edu/ralph/508S98/coordinates.html_qt_ rel_eq__qt_external nofollow_qt__gt_https_dd_//www.evl.uic.edu/ralph/508S98/coordinates.html_lt_/a_gt__co_ not the inversion of axises like Blender.  Left handed Z is positive going into the scene.  Right handed Z is positive coming out of the scene towards the viewer.  Wouldn_t_t that involve just switching the sign of Z?_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I clearly have multiple problems trying to test variable influencers_co_ make human_co_ cmu mocaps_co_ &amp_sm_ a before render based skeleton interpolator simultaneously.  I loaded one of the Acclaim skeletons into Blender_co_ and it is parallel to the ground.  This picture is in top view.  It looks like I should not have to do axis swapping._lt_/p_gt__lt_p_gt__lt_a class_eq__qt_ipsAttachLink ipsAttachLink_image_qt_ href_eq__qt_http_dd_//www.html5gamedevs.com/uploads/monthly_12_2015/post-8492-0-71431800-1449245514.png_qt_ rel_eq__qt_external nofollow_qt__gt__lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/monthly_12_2015/post-8492-0-71431800-1449245514.png_qt_ data-fileid_eq__qt_5733_qt_ class_eq__qt_ipsImage ipsImage_thumbnailed_qt_ alt_eq__qt_post-8492-0-71431800-1449245514.png_qt__gt__lt_/a_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I added a mocap animation to the skeleton_co_ and exported to compare to what I generate (Blend File_co_ _lt_a href_eq__qt_https_dd_//drive.google.com/file/d/0B6-s6ZjHyEwUUW96Zm1hbklNMDA/view?usp_eq_sharing_qt_ rel_eq__qt_external nofollow_qt__gt_https_dd_//drive.google.com/file/d/0B6-s6ZjHyEwUUW96Zm1hbklNMDA/view?usp_eq_sharing_lt_/a_gt_)_co_ if I can get the same frame for both (don_t_t ask).  I have so many problems right now_co_ I hope it will become obvious after I fix some_co_ what I need to do in this area._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-12-04T21:32:40Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_The very old Acclaim format uses earth_t_s magnetic coordinate system_co_ where Y and Z are flipped.  I recommend to always use the .fbx format_co_ and this will solve the orientation issue - depending on what program you export from as the older formats have legacy issues.  If you really want to solve all of your mocap issues_co_ I stream the Kinect V2 (doesn_t_t matter if it_t_s right hand or left hand) live into Motionbuilder_co_ and record there - then simply export a native FBX file.  This has been working for me since I purchased my V2 a year ago now._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-12-05T00:57:48Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Yeah_co_ I am starting to go off Acclaim.  The original Carnegie Mellon database seems to have a lot of bones flipped too.  Here is an original_co_ processed by the Acclaim importer for Blender_co_ then exported and played as a traditional Animation.  The wire frame is the ground.  It is oriented wrong and the hip is backwards._lt_/p_gt__lt_p_gt__lt_a class_eq__qt_ipsAttachLink ipsAttachLink_image_qt_ href_eq__qt_http_dd_//www.html5gamedevs.com/uploads/monthly_12_2015/post-8492-0-34306900-1449274069.png_qt_ rel_eq__qt_external nofollow_qt__gt__lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/monthly_12_2015/post-8492-0-34306900-1449274069.png_qt_ data-fileid_eq__qt_5734_qt_ class_eq__qt_ipsImage ipsImage_thumbnailed_qt_ alt_eq__qt_post-8492-0-34306900-1449274069.png_qt__gt__lt_/a_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I got a translation to bvh &amp_sm_ then _qt_corrected_qt_ from another website_co_ and loaded it using make walk_co_.  It ran perfectly.  Combination variable influencer &amp_sm_ vertex optimization with a skeleton testing is now complete.  Using separate meshes_co_ except for those which have shape keys_co_ allowed the feet to have 3_co_ the hair 5_co_ eyes 1_co_ clothes 7_co_ &amp_sm_ body 7 influencers.  Am putting mocap on hold for a few. I want to see if I specified an even lower number_co_ whether you could determine in the case of those verts which had more than the limit_co_ which were the least harmful to exclude. Right now it is first found_co_ first kept._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_When I say perfectly_co_ the left shoulder seems wrong_co_ but that is not the new vertex shader_t_s fault. If I morph a shape key before the animation though_co_ the face moves far away from the body_co_ but not in the Acclaim un-fixed version (initial translation was another fix). Will look to save out a mesh not in pose mode.  That should allow you to still have an animation in the export itself at index 0.  Translation is ok in the animation_co_ but roots translation will be converted to position change._lt_/p_gt__lt_p_gt__lt_a class_eq__qt_ipsAttachLink ipsAttachLink_image_qt_ href_eq__qt_http_dd_//www.html5gamedevs.com/uploads/monthly_12_2015/post-8492-0-48160700-1449275939.png_qt_ rel_eq__qt_external nofollow_qt__gt__lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/monthly_12_2015/post-8492-0-48160700-1449275939.png_qt_ data-fileid_eq__qt_5735_qt_ class_eq__qt_ipsImage ipsImage_thumbnailed_qt_ alt_eq__qt_post-8492-0-48160700-1449275939.png_qt__gt__lt_/a_gt__lt_/p_gt__lt_p_gt_Will look into .fbx.  Any other format is better than Acclaim for getting off the HD using a chooser_co_ since Acclaim format consists of 2 files_co_ not 1.  Do not have my own Kinect or MotionBuilder_co_ but this could be a benefit later.  Putting your animation in via another system and exporting is such a terrible work flow.  I am going to put a stop to it for the QueuedInterpolator extension._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-12-05T02:00:30Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_If your morphing is translating_co_ it is always a local center problem.  If you haven_t_t seen this before_co_ then make certain all of your morph target centers are aligned before setting any shape keys_co_ both locally and in world space - and it appears your already setting a default morph targt keyframe at frame 0 or 1 - I prefer 0.  You obviously understand that the .bvh file is a hierarchy animation file and not simply a file containing transformations.  It has always been too problematic to deal with these_co_ but if you look at the ascii .fbx format_co_ it_t_s much simpler to edit if you need to (which I haven_t_t had to do for many years_co_ as it is compatible with every program available these days.) I believe that .fbx is the gold standard to use for all scene elements prior to blender import and .babylon export._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"benoit-1842","Date":"2015-12-05T02:27:37Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi guys !!  If I may I will chime in in that very interesting subject.  First of all I think that using the Kinect v2 and doing mocap with it_co_ is way more complex then using the v1 (less litterature_co_ less program using it_co_ very heavy on ressource etc.).  I have use both and  I must say that for motion capture the accuracy (for my work) and avateering_co_ there`s not a lot of difference.  Me_co_ I am using the kinect 1 because I am working with high school students and it`s way cheaper and in a nutshell it`s fitting my bill.  This  year I have work a lot with the kinect 1 and I had awesome results with a little freeware call miku miku capture.  It motion capture in bvh and  that works perfectly with the famous makewalk load and retarget addon from makehuman that is used with Blender.  So in less then 1 minute I have a very good motion capture running in Blender with almost any biped using the makewalk addon.  Now I am facing a real dilemma.  The dilemma is when I export my animated character in babylon.js it takes 10 minutes to be converted has a .babylon file.  But when I am using blend4web to do the same task _co_ I have a html file of my animation in.......the push of a button (less then 2 seconds) I am in webgl land.  So I can have a homemade mocap animation in less then 2minutes.....  But babylon.js is fun to work with. If you want to try to take motion capture and avateering with a .fbx file I strongly suggest that you take a look at Live animation studio free version.  This little software is working well with fbx......  But me I am maybe a little bit old school and running on a very small budget_co_ efficacity and timeframe but after all my extensive work in that field _dd_     Mikumikucapture with the kinect 1 + makewalk (load and retarget in Blender) + blend4web is a good solution for me......  But if somebody is capable to do all those three steps in babylon.js I am all in to be part of it_co_ testing it and using it...._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Thanx_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_benoit_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-12-05T08:33:05Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Mikumiku capture is a fine pipeline for the V1 - but doesn_t_t work with the V2 - unless something has changed recently.  However_co_ for capture_co_ you are very right in pointing out that there is little difference in quality.  I use my V2 for scanning in higher resolution and it_t_s much faster_co_ so I use Brekel Body V2 as a plugin to Motionbuilder.  But for those people on the V1_co_ the pipeline you lid out works fine_co_ as I believe we discussed earlier this year.  As for te BVH file format_co_ it is being corrected in your pipeline to orient the transforms correctly.  I only recommend the Motionbuilder pipeline as there are so many tools that I only need to work in a single software - execpt for export from software that supports the .babylon format.  However_co_ there are standalone converters to do the job as well.  So I_t_m able to capture using the V2 and retargetting in real time on fully rendered characters - as well as puppeteering facial and other morph targets in real time.  _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Thanks for pointing this out_co_ as both pipelines work depending on what version of the Kinect you are using and what else you might need to support in your production._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"benoit-1842","Date":"2015-12-05T14:13:53Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Yeah your right ! My workflow is only using kinect v1 because in a high school environment we cannot use the v2 because of the price and you need a computer with usb 3 and at least 16 gig of ram ! I must say that for personnal use i use sometime the awesome Brekel software.  But me has a teacher and designer i am very surprise that the coder ninja in javascript-webgl aren_t_t really into motion capture and avateering with the kinect 1 or 2.  I will very prefer to do everything with webgl but for right now i think that everything cannot be done directly in webgl...so we have to use some offline technology...._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Polpatch","Date":"2015-12-05T17:22:53Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_div_gt_Thank you all for the answers!!  _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_biggrin.png_qt_ alt_eq__qt__dd_D_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/biggrin@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt_  _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_biggrin.png_qt_ alt_eq__qt__dd_D_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/biggrin@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/div_gt__lt_br_gt__lt_div_gt_But I_t_m working on a specific project and I can not use programs in background for animation in real time_co_ the kinect server for the acquisition of the frame is already an important compromise.. sigh_lt_/div_gt__lt_br_gt__lt_div_gt_I_t_ve got to move my avatar only with the information contained in the kinect frame_lt_/div_gt__lt_br_gt_\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-12-05T18:17:12Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_@Polpatch - Sorry for hijacking your thread_co_ but it I thought you were done with it._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_@DB - You are right_co_ I think_co_ but your explanation was a little mathy and does not even closely resemble the BJS pipeline.  If I may_co_ I will describe how I am achieving morph yet also using a skeleton_co_ so it will be more recognizable for most._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Positions / normals / UV / etc should be expressed relative to its local origin.  They are loaded into both a Float32Array on the cpu &amp_sm_ and a gpu buffer.  The vertex shader is passed up the current matix of each bone every call as attributes.  Each call of the vertex shader_co_ a vertex gets its defined starting position from the buffer_co_ and applies the influencers it has for itself.  This happens if it is animating or not_co_ so it is fixed overhead._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_When morphing_co_ I am changing the Float32Array &amp_sm_ refreshing the buffer.  This happens before either gpu or cpu skinning_co_ and I am morphing with the locally &amp_sm_ globally centered input to the skinning pipeline._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_What happened when I used makewalk to load a .bvh was it changed the vertices with the translation prior to even being exported_co_ so the base vertex data is corrupted.  See how the origin is now way in front of mesh.  It was right underneath it before.  Not sure if I can fix this with the exporter.  I guess_co_ just do not do that._lt_/p_gt__lt_p_gt__lt_a class_eq__qt_ipsAttachLink ipsAttachLink_image_qt_ href_eq__qt_http_dd_//www.html5gamedevs.com/uploads/monthly_12_2015/post-8492-0-45140800-1449337163.png_qt_ rel_eq__qt_external nofollow_qt__gt__lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/monthly_12_2015/post-8492-0-45140800-1449337163.png_qt_ data-fileid_eq__qt_5738_qt_ class_eq__qt_ipsImage ipsImage_thumbnailed_qt_ alt_eq__qt_post-8492-0-45140800-1449337163.png_qt__gt__lt_/a_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_@DB &amp_sm_ benoit - I am just wondering if it is possible to use a Kinect V1_co_ but with the motionbuilder work flow_co_ saving as a ascii fbx?  I do not have USB 3_co_ and do not need it for anything except capture.  Hope they still sell it.  It would nice to have work flow be as independent from HW as possible._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_As far as the export taking too long or motionbuilder not exporting a .babylon_co_  that is what I am talking about when I say the work flow is bad.  I am building tools which are bringing parts of the BJS development process into BJS_co_ and pulling them out of the export process.  So you make your mesh_co_ assign its materials_co_ skeleton_co_ and shape keys_co_ then export it.  You are done there._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_The voicesync or mocap app will either define your Arpabet+ strings &amp_sm_ audio_co_ or read your .fbx file and write out something that can be directly use in BJS_co_ without going back to the export system. The mocap tool will be something like makewalk.  makewalk only works with one input_co_ a .bvh.  I am in process of deciding what my input format(s) will be.  Both will have a stock character so you can see it actually operating in BJS as you develop. It may not be as good as makewalk in first release_co_ but you have to start somewhere._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"benoit-1842","Date":"2015-12-05T18:26:53Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi ! I think your input should be bvh like makewalk..... For me the famous makewalk is imho wonderful in collaboration with blender .And it works well with bvh and don_t_t forget that there code is open and accessible..... _lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-12-05T18:46:03Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_I have already separated my Mocap Class from AcclaimConverter Class_co_ trying to put as much as possible into the Mocap Class.  So I will not be locked in by design_co_ or can expand.  Acclaim looks not worth continuing with._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-12-05T18:56:51Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_@JC and @benoit-1832_dd_ You are both right from my experience in your approach and understanding.  Except JCPalmer appears to have skills beyond most_co_ and we_t_re all glad he_t_s working out these issues for the community.  As for the Brekel Motionbuilder pipeline_co_ Kinect V1 works fine - especially if you only require 30fps for animation - which is all that is required right now anyway.  Surface scanning using Kscan or another scanning program is much better with the V2_co_ however_co_ I don_t_t know anyone else on this forum who requires this._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_As for .bvh files_co_ it_t_s just important to know that this is almost a 30 year old format_co_ where Biovision made choices for compatability in the late 1980s.  But it was changed by the programmers at Giant Studios before I worked with them on films to make the .bvh compatable with the cartesian space we work in._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_As for mesh morphing_co_ it is all about centers and initial registered transforms as JCPalmer is clear_co_ and in my opinion_co_ the .bvh format has not been adapted well for vertex transforms - except again by the developers at Giant Studios - which they won_t_t share unless you know and work with them.  But for now_co_ this is what we have - which is why I hope more people begin to look at .fbx_co_ as this fomat was designed by Kaydara from the very beginning to replace the IGES format as a global standard.  If you want to look at the .fbx file_co_ there are plenty available to review_co_ and of course the binary .fbx is unreadable for editing text._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I look forward to what you guys develop for the community in the coming months._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-12-05T19:43:24Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_.fbx also holds a lot more than just skeletal animation_co_ I see.  Also I do not know what the FBX exporter in the Repo is.  Could that be plugged into MotionBuilder? I have not got a clue.  If it is a standalone executable_co_ it would seem it is a Windows program_co_ maybe to work with an SDK.  Wish it was a typescript parser I could steel._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I am only concerned with the skeleton part of the file._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_BTW_co_ might not another difference between V1 &amp_sm_ V2 be the # of bones per mesh tracked.  What skeleton definition is output?  Is there any .fbx known to be specifically make from a V2 stream somewhere?_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-12-06T23:24:52Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hey All_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I would also like to ask @Polpatch (who created this topic) if he believes these current discussions are relevant to his initial post.  I believe they are_co_ but wanted to make sure we_t_ve anwered his questions first - so @Polpatch_co_ please let us know if this has been helpful_co_ and if you still have issues which remain to be solved or understood._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_As for myself_co_ I feel a broader discussion on this topic is good for the community - and it_t_s providing me with allot to consider_co_ as well as probably the next series of posts on the pinned topic _qt_Spaces (world_co_ parent_co_ pivot_co_ local)_qt_ which was initially created by gwenael_co_ and is still read by a great # of users and visitors.  However_co_ having not been updated since July _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_sad.png_qt_ alt_eq__qt__dd_(_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/sad@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt_ _co_ this current topic on Kinect v2 animation provides me with quite allot of info to post - which is most likely good knowledge to reveal to the BJS community_co_ and keeps the info as a resource for everyone._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_So_co_ having said that -_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_@JCPalmer - It sounds as though you_t_ve opened an ascii .fbx file in a text editor_co_ which if true_co_ you can see that it holds an entire scene_t_s info including animation play controls and settings - and anything else you would find in practically any 3D software package.  It was developed by Andre Gauthier and his team at Kaydara as a new file format which was designed to and now does fully support most all 3D software production applications_sm_ allowing the import/export of practically all scene elements between applications.  On a very basic level_co_ you are able to export an .fbx file from Blender_co_ and then open that .fbx file containing the scene in most every other software application supporting the .fbx file format - and the initial Blender scene will function just as it did in Blender_sm_ including most all surfaces and complex objects_co_ image textures_co_ shaders (dependant)_co_ cameras_co_ lights_co_ skeletons_co_ animations_co_ vertex to bone binding - basically everything (for the most part)._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Now_co_ if you_t_re inquiring about the FbxExporter.exe on the GitHub Repo from the BabylonFBXNative Project_co_ it IS a standalone cmd line .exe.  But I don_t_t imagine this is what you are referring to_co_ as it is open source_co_ and you_t_d potentialy be able to use whatever elements_co_ functions_co_ operations_co_ etc. you might choose from it. I have some ideas as to what you might use this for_co_ however_co_ it_t_s not entirely clear to me what you_t_re ultimatte goal might be here.  But you have my complete attention at this time_co_ so perhaps sharing your thoughts and/or plans would be quite welcome - as I_t_ve seen what you are capable of_co_ and perhaps you_t_re just getting started.  In some of these areas_co_ I_t_m certain I could be of assistance. _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_wink.png_qt_ alt_eq__qt__sm_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/wink@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_As for the differences between the Kinect V1 and Kinect V2_co_ here_t_s a list of the key differences between the two models_dd__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_blockquote data-ipsquote_eq__qt__qt_ class_eq__qt_ipsQuote_qt__gt__lt_div_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt_QUICK REFERENCE_dd_ KINECT 1 VS KINECT 2_lt_div style_eq__qt_color_dd_rgb(182_co_182_co_182)_sm__qt__gt__lt_span_gt_Posted on _lt_/span_gt_ _lt_span_gt_March 5_co_ 2014_lt_/span_gt__lt_span_gt__lt_span_gt_Author _lt_span style_eq__qt_color_dd_rgb(117_co_153_co_197)_sm_background-color_dd_transparent_sm__qt__gt__lt_a href_eq__qt_http_dd_//www.imaginativeuniversal.com/blog/post/author/james-ashley_qt_ rel_eq__qt_external nofollow_qt__gt_James Ashley_lt_/a_gt__lt_/span_gt__lt_/span_gt__lt_/span_gt__lt_/div_gt__lt_div style_eq__qt_color_dd_rgb(182_co_182_co_182)_sm__qt__gt_ _lt_/div_gt__lt_div_gt_Feature                                                    Kinect for Windows 1                          Kinect for Windows 2_lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Color Camera                                          640 x 480 @30 fps                             1920 x 1080 @30 fps_lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Depth Camera                                         320 x 240                                            512 x 424_lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Max Depth Distance                                ~4.5 M                                                 8 M_lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Min Depth Distance                                 40 cm in near mode                            50 cm_lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Depth Horizontal Field of View                57 degrees                                          70 degrees_lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Depth Vertical Field of View                    43 degrees                                          60 degrees_lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Tilt Motor                                                 yes                                                       no_lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Skeleton Joints Defined                           20 joints                                               25 joints_lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Full Skeletons Tracked                             2                                                         6_lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_USB Standard                                         2.0                                                       3.0_lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Supported OS                                         Win 7_co_ Win 8                                        Win 8_lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Price                                                        $249                                                    $199_lt_/div_gt__lt_/div_gt__lt_/blockquote_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt__lt_em_gt_Additional Information_dd__lt_/em_gt__lt_/div_gt__lt_div_gt__lt_blockquote data-ipsquote_eq__qt__qt_ class_eq__qt_ipsQuote_qt__gt__lt_div_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_em_gt_“The Kinect v2 face recognition_co_ motion tracking_co_ and resolution are much more precise than the Kinect v1. Kinect v2 uses “time of flight” technology to determine the features and motion of certain objects. IGN summarized this technology well by comparing it to sonar technology_co_ except that this is a large improvement and more accurate. By using this technology_co_ the Kinect v2 can see just as well in a completely dark room as in a well lit room. Although the first Kinect used similar technology_co_ the Kinect v2 has greatly improved upon it.  The Kinect v2 has 1080 resolution (HD)_co_ and from the picture below you can see the difference between images._lt_/em_gt__lt_/p_gt__lt_p style_eq__qt_font-size_dd_13px_sm_color_dd_rgb(100_co_102_co_104)_sm_font-family_dd__t_Open Sans_t__co_ Arial_co_ sans-serif_sm__qt__gt__lt_a href_eq__qt_http_dd_//zugara.com/wp-content/uploads/Kinect-1-and-Kinect-2-Resolution-Comparison.png_qt_ rel_eq__qt_external nofollow_qt__gt__lt_img height_eq__qt_283_qt_ src_eq__qt_http_dd_//zugara.com/wp-content/uploads/Kinect-1-and-Kinect-2-Resolution-Comparison.png_qt_ width_eq__qt_467_qt_ alt_eq__qt_Kinect-1-and-Kinect-2-Resolution-Compari_qt__gt__lt_/a_gt__lt_/p_gt__lt_p style_eq__qt_font-size_dd_13px_sm_color_dd_rgb(100_co_102_co_104)_sm_font-family_dd__t_Open Sans_t__co_ Arial_co_ sans-serif_sm__qt__gt__lt_span style_eq__qt_color_dd_#000000_sm__qt__gt__lt_em_gt_Kinect v2 can process 2 gigabytes of data per second_co_ USB 3 provides almost 10x faster broadband for the data transfer_co_ 60% wider field of vision_co_ and can detect and track 20 joints from 6 people’s bodies _lt_/em_gt__lt_em_gt__lt_strong_gt_including thumbs_lt_/strong_gt__lt_/em_gt__lt_em_gt_. In comparison_co_ the Kinect v1 could only track 20 joints from 2 people. On top of this_co_ when using Kinect v2 we are capable of detecting heart rates_co_ facial expressions and weights on limbs_co_ along with much more extremely valuable biometric data. The Kinect v1.0 device doesn’t have the fidelity to individually track fingers and stretching and shrinking with hands and arms but the Kinect v2 has these capabilities. It’s clear that this technology is certainly much_co_ much_co_ more powerful and complex than the first generation of Kinect.”_lt_/em_gt__lt_/span_gt__lt_/p_gt__lt_/div_gt__lt_/blockquote_gt__lt_p_gt_ _lt_/p_gt__lt_/div_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I had to reformat these stinkin_t_ chart twice - many_co_ many minutes of my life just gone. Copy/Paste text simply blows. _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_angry.png_qt_ alt_eq__qt__dd_angry_dd__qt__gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Again_co_ I hope we hear from @Polpatch soon.  But the above info is good for everyone reading this_co_ as I see more and more users of the Kinect enter into the BJS pipeline weekly._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Cheers_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_DB_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-12-07T03:23:17Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi Polpatch_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I was re-reading your posts to try and understand what your goal actually is.  It appears that you want to simply animate your avatar using the Kinect in real-time as the avatar mesh is rendered in real-time.  If I_t_m not currect in my assumptions_co_ please let me know. But if I am_co_ then obviously your biggest problem is that this is nowhere near simple to accomplish._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_What I don_t_t understand is the following..._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_on 03 December 2015_co_ Polpatch wrote_dd__lt_/p_gt__lt_blockquote data-ipsquote_eq__qt__qt_ class_eq__qt_ipsQuote_qt__gt__lt_div_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_em_gt__lt_span style_eq__qt_font-size_dd_14px_sm__qt__gt__lt_span style_eq__qt_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt_I plan to use the Kinect v2 (that for xboxOne) to move an avatar in my scene._lt_/span_gt__lt_/span_gt__lt_/em_gt__lt_/p_gt__lt_div style_eq__qt_margin_dd_0px_sm_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt__lt_em_gt__lt_span style_eq__qt_font-size_dd_14px_sm__qt__gt_My idea was to gain the quaternions of jointOrientations and then change the appropriate bone matrix._lt_/span_gt__lt_/em_gt__lt_/div_gt__lt_div style_eq__qt_margin_dd_0px_sm_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt_ _lt_/div_gt__lt_div style_eq__qt_margin_dd_0px_sm_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt__lt_em_gt__lt_span style_eq__qt_font-size_dd_14px_sm__qt__gt_Nevertheless_co_ the jointOrientations are expressed in global rotations (each jointOrientation indicates the direction of the joint in absolute coordinates) while_co_ if I understand it_co_ I can only modify the local matrix of skeleton bone._lt_br_gt__lt_br_gt_So I am trying to convert local jointOrientation in local rotation_dd__lt_/span_gt__lt_/em_gt__lt_/div_gt__lt_div style_eq__qt_margin_dd_0px_sm_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt_ _lt_/div_gt__lt_div style_eq__qt_margin_dd_0px_sm_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt__lt_div style_eq__qt_margin_dd_0px_sm__qt__gt__lt_span style_eq__qt_font-size_dd_14px_sm__qt__gt__lt_span style_eq__qt_color_dd_rgb(0_co_0_co_136)_sm__qt__gt_var_lt_/span_gt__lt_span_gt_ joint_lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt__sm__lt_/span_gt__lt_span_gt_ _lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(136_co_0_co_0)_sm__qt__gt_//the joint of kinect_lt_/span_gt__lt_br_gt__lt_span style_eq__qt_color_dd_rgb(0_co_0_co_136)_sm__qt__gt_var_lt_/span_gt__lt_span_gt_ parent _lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt__eq__lt_/span_gt__lt_span_gt_ joint_lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt_._lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_0_co_102)_sm__qt__gt_Parent_lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt_()_sm__lt_/span_gt__lt_br_gt__lt_br_gt__lt_span style_eq__qt_color_dd_rgb(0_co_0_co_136)_sm__qt__gt_var_lt_/span_gt__lt_span_gt_ localOrientation _lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt__eq__lt_/span_gt__lt_span_gt_ BABYLON_lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt_._lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_0_co_102)_sm__qt__gt_Quaternion_lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt_._lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_0_co_102)_sm__qt__gt_Inverse_lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt_(_lt_/span_gt__lt_span_gt_parent_lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt_._lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_0_co_102)_sm__qt__gt_Orientation_lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt_)._lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_0_co_102)_sm__qt__gt_Multiply_lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt_(_lt_/span_gt__lt_span_gt_joint_lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt_._lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_0_co_102)_sm__qt__gt_Orientation_lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(102_co_102_co_0)_sm__qt__gt_)_sm__lt_/span_gt__lt_/span_gt__lt_/div_gt__lt_div style_eq__qt_margin_dd_0px_sm__qt__gt_ _lt_/div_gt__lt_p_gt__lt_em_gt__lt_span style_eq__qt_font-size_dd_14px_sm__qt__gt_But I_t_m having trouble in the transformation of the reference coordinate between kinect joints and avatar bones int the babylon scene..._lt_/span_gt__lt_/em_gt__lt_/p_gt__lt_p_gt__lt_em_gt__lt_span style_eq__qt_font-size_dd_14px_sm__qt__gt_I tried to change the axes by swapping values (x_co_ y_co_ z)_co_ but I_t_m probably wrong_lt_/span_gt__lt_/em_gt__lt_/p_gt__lt_/div_gt__lt_/div_gt__lt_/blockquote_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_It would be very helpful to understand all of the elements you have in your scene at this time.  Are you currently reading each bone center_t_s orientation (x_co_y_co_z) from the Kinect as it runs in real time and attempting to map this onto a seperate bone (retargetting)?  If so_co_ there are far too many issues with this to even list in this thread.  Without more code available to review_co_ as well as a list of scene elements and a more thorough description_co_ it is practically impossible for me to suggest where you might begin to run some tests to identify your problems - which I would have to assume are many at this time._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I have been working with motion capture on high profile productions for more than 20 years_co_ and am sure I have faced exactly what you are up against now.  Do you have much experience in motion capture_co_ and if so_co_ what systems are you proficient using and/or have used in the past?  This is important_co_ as each system has it_t_s own methods of aquisition and retargetting onto a secondary skeleton. If you have none or not much experience in mocap_co_ then there may be layers of problems you are currently facing.  So knowing all of the above is the best way for me to personally assist._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_But_co_ I_t_ll at least try and provide some suggestions until we have more information.  Have you been able to send messages such as using the console.log() for the (x_co_y_co_z) positions and rotations of a single Kinect bone_t_s center at a decreased fps - such as 5 fps or 10fps?  If it were me_co_ I would begin by writing a function to do this first to compare these values against the values reported from the Kinect SDK.  This should tell you that at least the transforms for that single bone are read correctly into the BJS scene in real time._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_If the values appear the same_co_ then I would probably create a cube in BJS_co_ and constrain the cube_t_s local center position to the single Kinect bone_t_s (x_co_y_co_z) position.  But matching the orientation of the cube to the exact orientation of the Kinect bone may or may not be easy.  However_co_ I would first just try and animate the cube_t_s rotation using a version of the previous function which attenuates the cube_t_s fps from 30fps to 5fps - and simply try and rotate the cube (animate the rotation of the cube in real-time) using the (x_co_y_co_z) rotations from the single bone of the Kinect skeleton.  Also_co_ you need to set a distinct texture on the cube so that you will clearly be able to see how the cube is rotating as well as any undesired behaviors (transforms.)_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I_t_m guessing that you are already way past this point and know what numeric values are being read into your BJS scene currently.  I_t_m guessing this because you appear to have already seen a delta in your bone rotations_co_ since it appears from your brief sample code that you we_t_re attempting to re-order your axis matrices_t_ (x_co_y_co_z) on your bones in BJS already. But I can_t_t assume anything unless I have a whole lot more info. So I_t_m advising as though you just began to attempt streaming your mocap data into your BJS scene_co_ and have no or little experience using mocap at all.  If we were working in the same room together_co_ and providing you are already streaming the Kinect data into a Babylon.js scene_co_ I_t_m confident that we could solve this in a few hours at most_co_ since this was much more difficult to accomplish when I first faced this problem in the mid 90s.  I should be a walk in the park today._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_So as I_t_m writing this_co_ again_co_ I_t_ll guess that you do have some experience in mocap_co_ you know the (x_co_y_co_z) values of the data streaming from the Kinect into your BJS scene_co_ you have already imported a copy of your Kinect skeleton into your Babylon scene_co_ and using some function_co_ have attempted to directly map the Kinect data onto the copy of the skeleton - otherwise_co_ why would you want to re-order your bone_t_s matrices_t_._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_So_co_ I_t_ve given you both extremes - one_co_ in which you are a most ambitious person who_t_s attempting this with little experience_sm_ and the other extreme_co_ where you already have an exact copy of your Kinect skeleton rendering in a Babylon scene and are viewing an incorrect re-targetting of the Kinect data in real time.  I_t_m guessing the latter_co_ however_co_ I still can assume nothing.  If you are already driving a skeleton with your Kinect V2 in real-time in BJS_co_ then it may be as simple as freezing all transforms on your babylon skeleton and applying offsets (probably not that simple_co_ but not that difficult either). So I and others can certainly help you figure this out either way_co_ however_co_ I would need considerably more information on all fronts to assist.  If I were to receive the info requested_co_ and you do have the experience I believe you might have_co_ then you will be the first to accomplish this_co_ and it shouldn_t_t be a huge effort to do so.  I hope I_t_m right._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Cheers_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_DB_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Polpatch","Date":"2015-12-07T12:49:07Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_@_lt_a href_eq__qt_http_dd_//www.html5gamedevs.com/user/8492-jcpalmer/_qt_ title_eq__qt__qt__gt_JCPalmer_lt_/a_gt_ no problem _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_wink.png_qt_ alt_eq__qt__sm_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/wink@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt_ ahahah_lt_/p_gt__lt_p_gt_Hi Dbawel_co__lt_/p_gt__lt_div_gt_first of all I want to thank you for the great help that you give me._lt_/div_gt__lt_div_gt_i have almost no experience_co_ just in these months I_t_m dipping (or drowning hahahah) in this area._lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_I use a local server to send frames kinect to my project via socket._lt_/div_gt__lt_div_gt_For each frame_co_ I store the original information of each joint in an appropriate object. This object provides a match between joint kinect and skeletal bones of Babylon_co_ the hierarchical map of the skeleton kinect and rotation/position of the joints (use only the position of the joint spineBase for the global position)._lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_The _qt_pseudo-code_qt_ you mentioned has the task to get the local rotation to be applied to the bone of my skeleton_co_ since I can not apply global changes in the bones Babylonjs (or at least I have not found anything)._lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_I tried to study the behavior of a joint orientation to change the reference coordinates and then use the data directly into the bone (with the appropriate corrections to correct the offset between the two skeletons. But for every bone I am obliged to further modify the axes_co_ since the rotations did not correspond to the rotations of the joint kinect._lt_/div_gt__lt_div_gt_I have now found the right combination to match the right forearm of the two skeletons_co_ but over the weekend I was not able work to find the combination of other bones_co_ I will try this afternoon  _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_biggrin.png_qt_ alt_eq__qt__dd_D_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/biggrin@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/div_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-12-07T19:51:18Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_@db_co_  I just happened to see the FBX exporter problem thread.  Had no idea what it did_co_ saw it was not in typescript (meh)_co_ but thought you might use it_co_ if you did not know about it._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I have not actually seen a text based FBX_co_ I just inferred it was more than a mocap file based on Wikipedia_co_ &amp_sm_ some unofficial Blender documentation on FBX format.  I am seeing all binary fbx_t_s_co_ at least for the cmu skeleton.  Now that you mentioned Blender could write a .fbx_co_ I thought I use to get a .fbx from the .bvh to test with_co_ but Blender generates a binary .fbx._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Was already doing a bone name translation from cmu names to make human_t_s for Acclaim.  Think anything more than a rename is going to be too much to do.  Remember_co_ I am not just running this using scene.animate().  Too many places for problems to occur not to have a 1_dd_1 bone match up right now.  A binary .fbx is also doable_co_ but not really a good choice until process debugged with a text format._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Will think about it as I start putting together a release of the Blender exporter.  Have many changes there_co_ &amp_sm_ want to finish something.  I already have multiple classes (Mocap separate from input classes). Think .bvh could be parsed in about 8 hours.  Could be the easiest way to get this over the hump._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Kinect data interesting_co_ V2 costs less_co_ not more.  My concern here is not cutting myself off now_co_ for something I am likely to want to do in the future._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-12-08T04:21:10Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_@ JCPalmer - I_t_m actually writing this bit last as I NEVER expected to get into the explaination I delivered below. So as my memory is blown now_co_ I_t_ll read your post again tommorrow - but I did want to tell you that I can write out an ascii .fbx file for you if you like. I have Motionbuilder_co_ so it is no problem for me to do so. Let me know._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_@Polpatch - OK_co_ your last response tells me allot_sm_ but there_t_s still many things to understand._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_div style_eq__qt_margin_dd_0px_sm_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt__lt_blockquote data-ipsquote_eq__qt__qt_ class_eq__qt_ipsQuote_qt__gt__lt_div_gt__lt_div style_eq__qt_margin_dd_0px_sm_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt_I use a local server to send frames kinect to my project via socket._lt_/div_gt__lt_div style_eq__qt_margin_dd_0px_sm_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt_For each frame_co_ I store the original information of each joint in an appropriate object. This object provides a match between joint kinect and skeletal bones of Babylon_co_ the hierarchical map of the skeleton kinect and rotation/position of the joints (use only the position of the joint spineBase for the global position)._lt_/div_gt__lt_p_gt_ _lt_/p_gt__lt_/div_gt__lt_/blockquote_gt__lt_p_gt_1. I_t_d love to see the code using _qt_socket_qt_ (WebsocketIO - I assume) which stores the joint info into an object - and what type of object is this? A null center?_lt_/p_gt__lt_p_gt_2. Where did you generate the target skeleton from - Blender_co_ Max_co_ Maya_co_ another program? Does your target skeleton contain the same # and topography of the Kinect skeleton?_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I have many more questions_co_ however_co_ I can tell you now that you will have many_co_ many problems to overcome in retargetting mocap data in the way you are trying to do at this time. I can also say that you are very brave and bold to attempt this with little experience in motion capture. The approach to setting up reliable retargetting will be entirely in your pipeline to prepare your target skeleton to receive real-time animation from a source which is as much about animation_co_ as it is the initial transforms of the bones_co_ and both skeleton_t_s hierarchies._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_In order to minimize all of the problems you_t_ll definitely encounter_co_ for your first tests your target skeleton must be an exact copy of the Kinect skeleton in all attributes - hierarchy_co_ bone topography_co_ bone position_co_ bone orientation (rotations)_co_ and bone scaling (which must be a value of 1.0 on all axis.) This is why I asked how/where you generated your target skeleton. As I had asked what is the object in which you store the Kinect data_co_ I would ask someone like DeltaKosh if it_t_s possible to generate bones in BJS from nothing_sm_ as this would be the very best of solutions. I read a post in the past where DK appeared to say that this was possible_dd__lt_/p_gt__lt_p_gt__lt_a href_eq__qt_http_dd_//www.html5gamedevs.com/topic/16326-how-to-create-skeleton-and-bone-with-out-using-blender/_qt__gt_http_dd_//www.html5gamedevs.com/topic/16326-how-to-create-skeleton-and-bone-with-out-using-blender/_lt_/a_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_However_co_ I have yet to find a way to do this myself. But if this is possible_co_ then this might be valuable to the process. However_co_ in the long run_co_ the end result must be that you are able to drive a skeleton with a mesh weighted to the bones_sm_ and that is where the challenge truly is. So right now_co_ I would only focus on driving an exact copy of the Kinect skeleton in Babylon and make certain that the process is completely reliable prior to attempting any retargetting to a skeleton with a mesh attached and/or a different skeleton. One step at a time_co_ right?_lt_/p_gt__lt_p_gt__lt_span_gt_When you have this completely debugged_co_ then look at offsetting orientation_co_ position_co_ and topography - which all three of these are going to be huge hurdles to achieve._lt_/span_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Also_co_ before there ever existed any retargetting functions or applications (20 years ago - I_t_m really dating myself now)_co_ I would accomplish this through parenting and constraints. I recommend using this process yourself_co_ as this will provide you with the very best results using what is currently available in BJS._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_span_gt_First_co_ we should talk about the _qt_Neutral Pose._qt_ In order for any of this to work (at least without a whole bunch of other functions_co_) you must have already set your target skeleton at a unique stationary stance or pose.  This is normally with the target skeleton facing -Z in world space as the Kinect skeleton should be. But just make certain that the target skeleton is facing the very same direction in world space as the Kinect skeleton is in world space. Now_co_ it_t_s important that when you create or export your target skeleton_co_ that it is in a specific body pose. This would be with the body completely standing straight_co_ both legs straight and together - but not touching_co_ as the feet should be approximately 6 - 8 inches apart and facing straight from toe to heel in the same direction as the body (of course.) Then the arms must be straight out from the sides at a 90 degree angle and the palms of the hands also flush to the ground and straight out from the arms. This is the Neutral Pose. It is key to making the entire system work for you_co_ and is still used in every motion capture session today. Don_t_t be confused by the Kinect not requiring this_co_ as Microsoft uses algorithms based upon human physiology to avoid making the user do this. But your users must do this - at least for now._lt_/span_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Next step - create boxes (cubes) at the center of each bone of your target skeleton. Give these a consistant name so that you know what these are - such as _qt_source_jointname_qt_ where jointname is the name of the target bone. Copy each one of these boxes_co_ and name these _qt_target_jointname_qt__co_ making certain that these target boxes are in the exact position of the source boxes which are in position on the target skeleton. Then you must change (set) the rotation of each of the target boxes to match the rotation of each cooresponding joint (where each of these boxes where created). _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_So_co_ now you have a target skeleton in a neutral pose with two boxes at each bone_t_s joint center - and the _qt_target_qt_ boxes have an (x_co_y_co_z) rotation value matching each cooresponding joint_t_s rotation exactly._lt_/p_gt__lt_p_gt_Stay with me now._lt_/p_gt__lt_p_gt_The next step is to capture a single frame of your source skeleton_co_ or with the Kinect_co_ you could record this ahead of time as the Kinect wil be consistant per user - but not universally. You must use a single frame from the source skeleton recorded for each user_co_ so it would be best to build this into a function using the Kinect SDK to capture a single frame which then must be used to set up the next step in this process to correctly retarget mocap data on the most basic level. Of course_co_ when this single frame is captured_co_ the person must be in the _qt_Neutral Pose_qt_ just as the target skeleton is set._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Once you have the single frame of data from the Kinect which represents your neutral pose from the source skeleton_co_ this must be accessible in your running babylon scene. What must be done now is to change the rotation of every _qt_source_qt_ box to the (x_co_y_co_z) rotation value from the source skeleton at each cooresponding bone (joint.) Now you have two sets of boxes_sm_ both sets in the position of each cooresponding joint of the target skeleton_co_ with the rotation value of each _qt_source_qt_ box the same as the rotation value of each cooresponding joint on the source skeleton_co_ and the_lt_span_gt_ rotation value of each _qt_target_qt_ box the same as the rotation value of each cooresponding joint on the target skeleton. We_t_re almost there._lt_/span_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_span_gt_Now before we can begin driving the target skeleton with the data from the Kinect_co_ you must parent and constrain objects in the scene exactly in the following manner_dd__lt_/span_gt__lt_/p_gt__lt_p_gt__lt_span_gt_1. Make each _qt_target_qt_ box a child of each _qt_source_qt_ box at it_t_s location. And make certain that you don_t_t place these stps anywhere else in the process. It is the parenting that will now very acurately maintain the correct offset in rotations between your source skeleton and target skeleton - and without any further computations_co_ however since these boxes are children_co_ their position and rotation values can be later changed to provide additional local offsets for the data driving each bone at the source boxes. This will make more sense once you completely understand this entire process._lt_/span_gt__lt_/p_gt__lt_p_gt__lt_span_gt_2. Constrain both the position and orientation of each bone in the target skeleton to each cooresponding _qt_target_qt_ box at each bone_t_s position. As each bone has the exact same position and rotation values as their cooresponding target box_co_ there should be no change in transforms on the target skeleton whatsoever. If you see that there is_co_ then you must have made a mistake at some point in the process._lt_/span_gt__lt_/p_gt__lt_p_gt__lt_span_gt_3. Now in the next step (really many steps)_co_ you need to have a complete understanding of this entire process to avoid making a mistake and finding complete failure. The goal is to drive the position and rotation of each source box by the data from each cooresponding joint/bone in the Kinect skeleton. For this system to work_co_ you must create a hierarchy within your _qt_source_qt_ boxes which EXACTLY matches the hierarchy of your source skeleton. And we must also assume that the hierarchy of your target skeleton already matches the hierarchy of your source skeleton as well - which needs to exist in any successful retargetting of data using any application. The hierarchy of your target skeleton needs to have already been set in an external application - as we_t_ll assume the skeleton was exported and converted into a .babylon file. So be careful to make sure you know very well the parenting in your source (Kinect) skeleton before you begin setting up this entire process._lt_/span_gt__lt_/p_gt__lt_p_gt__lt_span_gt_So for your _qt_source_qt_ boxes_co_ as common sense dictates_co_ wherever the root parent is in the Kinect skeleton_co_ you must _lt_strong_gt_now create an additional box_lt_/strong_gt_ which we will name _qt_source_root_qt_. This _qt_source_root_qt_ box needs to be the top parent of all of the the source boxes_co_ and this _qt_source_root_qt_ box will only be used for translation_co_ not rotation. I haven_t_t looked at the Kinect skeleton in some time_co_ but if its root parent is at the base of the spine - or on the pelvis_co_ set the source_root box position to be exactly where ever the root of your source skeleton is located in world space. then parent all other source boxes to their cooresponding parent as represented in the Kinect skeleton._lt_/span_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_So with this set up_co_ you should have everything in place to drive your retargetted skeleton. Send the root position data from your object which holds the Kinect root transforms to the _qt_source_root_qt_ box_co_ and only the position data. For the rest of the skeleton_co_ you will only send rotation data from each object which holds the transforms for each cooresponding bone in the Kinect skeleton to its cooresponding _qt_source_qt_ box - and not any positional data. Using this setup allows a _qt_zero_qt_ offset between the source boxes and the target boxes which means that the data is automatically offset without you having to make any adjustments. This also solves flipping issues in your target skeleton which is always a huge issue and practically inmpossible to solve - normally - because the bones in your target skeleton don_t_t reach rotational values &gt_sm_179 or &lt_sm_ -179_co_ providing you freeze all transforms on your target skeleton before you export your skeleton to a .babylon file. Always freeze your transforms on all matrices_t_ for any skeleton you export - for mocap or any animation._lt_/p_gt__lt_p_gt_An additional benefit is that you will also be able to set additional offsets to correct any undesired behaviors in your target skeleton by rotating and or translating your target cubes to be in a different position or orientation from its _qt_zeo offset_qt_ initial transforms - and this can be done in real time_co_ dynamically_co_ and conditionally if necessary or desired - although do this with great caution_co_ and in very small amounts._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I still suggest you begin incrementally_co_ and make certain that you can drive a target skeleton exported straight from the Kinect _lt_span_gt_with the exact transforms as the Kinect skeleton _lt_/span_gt__lt_span_gt_(it_t_s not that straight forward_co_ but I hope you follow what I_t_m suggesting.)  And I certainly don_t_t suggest that you build this as a first attempt in doing so_co_ as it will take time to fully understand this setup and why it works - as well as all of the benefits in using this method. This is very similar to what any retargetting software is doing which is on the market today. It_t_s simply that this setup and the functions driving it are hidden from the user and not really accessible. Of course_co_ once you_t_re familiar with the process_co_ all of these steps can be scripted_co_ so the process can be fast and optimized considerably._lt_/span_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_span_gt_I certainly didn_t_t expect to get into this on this post_co_ as it will take a great deal of thought on everyone_t_s part to really comprehent all of the relationships which are happening in the process I_t_ve described above - but consider that I had to figure this out when no one had yet heard of motion capture_co_ and there were no tools to even make use of the data. So for those of you who actually read this and can understand the process_co_ I hope it will also provide a greater insight into other aspects of animation and center axis transforms. And next time you use Motionbuilder_co_ yo_t_ll know what_t_s going on under the hood. It was first named Filmbox_co_ and I bought the very first license prior to this when it was expensive stage lighting control software - and Kaydara added objects so that we could control objects with channel animation - so I hope you get the idea._lt_/span_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_span_gt_I think that_t_s enough._lt_/span_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_span_gt_Cheers_co__lt_/span_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_span_gt_DB_lt_/span_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_OK_co_ I thought I might add that the only way to really grasp this without driving yourself mad_co_ is to diagram this out BEFORE you attempt putting this process into place. There is never a replacement for pencil and paper.  _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_wink.png_qt_ alt_eq__qt__sm_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/wink@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_/div_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-12-08T23:11:40Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_@JCPalmer - Great news! I can_t_t believe that I didn_t_t know about this tool_co_ but Autodesk has a free tool called FBX Converter_co_ and I believe that v2013.3 is the latest version. I downloaded and installed_co_ and realized there are many nice features in this application. It has a windows based UI to convert several file formats to .fbx files - both binary and ascii. It also allows you to convert binary .fbx files to ascii .fbx files and in reverse - so this will allow you to easily convert to ascii .fbx and edit. The main limitation is that the other file formats that it supports are few_co_ however_co_ there are many converters available - so the real value here is that you can use the FBX Converter to produce an ascii .fbx and edit or use as text_co_ and then convert back to binary_co_ as blender and other application primarily support only the binary .fbx format._lt_/p_gt__lt_p_gt_The reason for this is that it is very quick to parse and load the binary format_co_ but since the FBX format supports practically EVERY aspect of a scene_sm_ and there is so much information in the ascii format that these are large files and take longer to load._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_However_co_ in addition there are other great features such as a viewer_co_ where you can quickly view your fully rendered scene with shaders and textures turned on - or turn on and off practically every aspect of shading and model_co_ as well as control elements such as skeletons. The viewer also accepts all channel inputs in the scene such as controllers and any device supported by Motionbuilder - which are many. This allows you to puppeteer in real time without the need to launch the overhead of Motionbuilder on your system. And again_co_ it_t_s free._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_So as I_t_ve mentioned_co_ open an ascii .fbx file (the converter installs with several sample scenes) and you_t_ll see how vast the support of scene elements are - as well as how easy it is to read and edit. Editing FBX files in past productions has saved me countless hours of work as well as being able to script repetitive edits - and has allowed me to do things I could do in my 3D software application itself. Well worth checking out._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-12-08T23:38:21Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Yea_co_ looks good.  I am done for the day for the east coast.  In my efforts to get the next version of the Blender exporters ready for production_co_ this was out of my consciousness.  I usually know shortly what to do after working on something else for a while (power nap_co_ when really tight for time).  Figured out I should be able find at least one .bvh using cmu skeleton with no translation baked into vertices._lt_/p_gt__lt_ol_gt__lt_li_gt_ I can load it with makewalk_co__lt_/li_gt_\t_lt_li_gt_then delete the meshes.  _lt_/li_gt_\t_lt_li_gt_Export into binary .fbx._lt_/li_gt__lt_/ol_gt__lt_p_gt_Was then going to ask you if you could convert to ascii.  Looks like I can do it myself.  Thanks.  I have both Mac &amp_sm_ Linux machines.  One should work._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-12-09T00:59:39Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Thanks for all of your work on the exporter. The whole community including me uses it almost everyday. I_t_m with you - my brain fries at a some point each day - I don_t_t know how you put so much effort into this after other work (I assume)._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I hope you find some good info and usage from the .fbx format - definately worth looking at_co_ since it holds so much scene info and is compatible across so many platforms._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Polpatch","Date":"2015-12-09T17:51:20Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_div_gt_Hi @dbawel and thank you for all the time you are devoting to me!_lt_/div_gt__lt_div_gt_Regarding your question_dd__lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_1.Yes_co_ the server Kinect (github_dd_ _lt_a href_eq__qt_https_dd_//github.com/wouterverweirder/kinect2_qt_ rel_eq__qt_external nofollow_qt__gt_https_dd_//github.com/wouterverweirder/kinect2_lt_/a_gt_) uses exactly WebSocketIO. _lt_/div_gt__lt_div_gt_The server does not have all the features of the Kinect SDK native (eg missing the status of joint)_co_ but I can easily make changes._lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_The server sends the following types of frame_dd__lt_/div_gt__lt_div_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_Kinect2.FrameType _eq_ {        none\t\t\t\t_dd_ 0_co_        infrared\t\t\t_dd_ 0x2_co_ //Not Implemented Yet\tlongExposureInfrared\t\t_dd_ 0x4_co_ //Not Implemented Yet\tdepth\t\t\t\t_dd_ 0x8_co_\tbodyIndex\t\t\t_dd_ 0x10_co_ //Not Implemented Yet\tbody\t\t\t\t_dd_ 0x20_co_\taudio\t\t\t\t_dd_ 0x40_co_ //Not Implemented Yet\tbodyIndexColor\t\t\t_dd_ 0x80_co_\tbodyIndexDepth\t\t\t_dd_ 0x10_co_ //Same as BodyIndex\tbodyIndexInfrared\t\t_dd_ 0x100_co_ //Not Implemented Yet\tbodyIndexLongExposureInfrared\t_dd_ 0x200_co_ //Not Implemented Yet\trawDepth\t\t\t_dd_ 0x400_co_\tdepthColor\t\t\t_dd_ 0x800}_sm__lt_/pre_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_In particular the structure of the frame-type body (and all its components)_dd__lt_/div_gt__lt_div_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_//bodyFramebodies_dd_ Array[6]_sm_floorClipPlane_dd_ Quaternion_sm_//body objectbodyIndex_dd_ int_sm_joints_dd_ Array[25]_sm_leftHandState_dd_ int_sm_rightHandState_dd_ int_sm_tracked_dd_ bool_sm_trackingId_dd_ int_sm_//joint objectcameraX_dd_ double_sm_cameraY_dd_ double_sm_cameraZ_dd_ double_sm_coloX_dd_ double_sm_colorY_dd_ double_sm_depthX_dd_ double_sm_depthY_dd_ double_sm_orientationX_dd_ double_sm_orientationY_dd_ double_sm_orientationZ_dd_ double_sm_orientationW_dd_ double_sm__lt_/pre_gt__lt_div_gt_Currently I just memorize (orientationX_co_ orientationY_co_ orientationZ_co_ orientationW) of each joint._lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_2. Avatar that I use is a native of blender (_lt_a href_eq__qt_http_dd_//blog.machinimatrix.org/avatar-workbench/_qt_ rel_eq__qt_external nofollow_qt__gt_http_dd_//blog.machinimatrix.org/avatar-workbench/_lt_/a_gt_) and the initial pose coincides (luckily) with the one you described. _lt_/div_gt__lt_div_gt__lt_img src_eq__qt_http_dd_//s30.postimg.org/f25qs8bz5/avatar_BJS.png_qt_ alt_eq__qt_avatar_BJS.png_qt__gt__lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_I think I understand in outline the process that you have explained_co_ now I have to study it thoroughly._lt_/div_gt__lt_div_gt_Just one thing_co__lt_/div_gt__lt_div_gt__lt_blockquote data-ipsquote_eq__qt__qt_ class_eq__qt_ipsQuote_qt__gt__lt_div_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_span style_eq__qt_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt_Next step - create boxes (cubes) at the center of each bone of your target skeleton. _lt_/span_gt__lt_/div_gt__lt_/blockquote_gt__lt_p_gt_Taking as an example the forearms_co_ saying _qt_at the center of each bone_qt_ you mean that the cube should be placed between the elbow and wrist_co_ or exactly in the corresponding joint (in the case of the Kinect in the wrist)?_lt_/p_gt__lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_In my ignorance I thought was enough to read the orientation of the joint kinect and add it (with brute force hahaha) to the bone corresponding with the necessary corrections to the rotation._lt_/div_gt__lt_div_gt_Undoubtledly follow your advice._lt_/div_gt__lt_div_gt_ _lt_/div_gt__lt_div_gt_Thank you so much_co_ I will keep you updated in the coming days!! I owe you a  lot of coffee _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_biggrin.png_qt_ alt_eq__qt__dd_D_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/biggrin@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/div_gt__lt_/div_gt__lt_/div_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"}]