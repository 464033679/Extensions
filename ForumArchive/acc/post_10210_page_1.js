[{"Owner":"JCPalmer","Date":"2014-11-04T15:08:44Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_DK_co__lt_/p_gt__lt_p_gt_After looking in repository &amp_sm_ seeing that the new builds were for 2.0_co_ I started digging into what is in it.  Saw WebAudio._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Is this just playback_co_ or will you be able to record? I already have a use for that!_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Cocoon also has sound playback_co_ could that be called on detection?  I have completed Cocoon tolerance changes_co_ but not put in pull request.  Was going to do capabilities testing_co_ but maybe should just get it up there &amp_sm_ let others do some of the work._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Jeff_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"davrous","Date":"2014-11-04T16:32:58Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ I_t_m in charge of the Web audio engine. I_t_ll focus myself on the sound playback. Recording needs the usage of the Media Capture API. Why would you need sound recording in a game?_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Bye_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_David_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Xeonzinc","Date":"2014-11-04T18:28:08Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Someone might want to create a singstar style game_co_ like _lt_a href_eq__qt_http_dd_//www.karaokeparty.com/_qt_ rel_eq__qt_external nofollow_qt__gt_http_dd_//www.karaokeparty.com/_lt_/a_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Or more generally_co_ voice chat within the game?_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2014-11-04T18:28:11Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hey_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_My application is a game development tool_co_ not a game itself._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_a href_eq__qt_https_dd_//googledrive.com/host/0B6-s6ZjHyEwUSDVBUGpHOXdtbHc_qt_ rel_eq__qt_external nofollow_qt__gt_https_dd_//googledrive.com/host/0B6-s6ZjHyEwUSDVBUGpHOXdtbHc_lt_/a_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_The idea is to make game characters talk.  It is very difficult syncing meshes to audio.  Going from the other direction of syncing audio to meshes just means a little practice looking at the mesh in the tool.  Pre-recorded audio is also likely copyrighted_co_ so making your own audio is also free from issues._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_This tool does look ups of sentences in Carnegie Mellon University_t_s_co_ DARPA funded_co_ Phoneme database.  I wrote a Java program which converts it to a JavaScript module.  The database is big_co_ but you will only need it in the tools_co_ not the final application._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_It is not really done_co_ but getting there.  Will be adding a check box for starting a count down.  You could use a phone to make the actual recording_co_ but might be less fumble_co_ if the tool could do it._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Jeff_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2014-11-04T21:45:13Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_After your comment on _qt_Media Capture API_qt__co_ I have been doing some searching.  Here are some notes for myself_co_ or anyone who might stumble on this_dd__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_ol_gt__lt_li_gt_See Media Capture API here_dd_ _lt_a href_eq__qt_http_dd_//www.w3.org/TR/html-media-capture/_qt_ rel_eq__qt_external nofollow_qt__gt_http_dd_//www.w3.org/TR/html-media-capture/_lt_/a_gt_ (&lt_sm_form tag&gt_sm_ based. ok for starting_co_ but how to stop in code?  Is this is implemented anywhere? No need for integration into BJS)_lt_/li_gt_\t_lt_li_gt_There is the navigator.getUserMedia() here_dd_ _lt_a href_eq__qt_http_dd_//w3c.github.io/mediacapture-main/getusermedia.html_qt_ rel_eq__qt_external nofollow_qt__gt_http_dd_//w3c.github.io/mediacapture-main/getusermedia.html_lt_/a_gt_ &amp_sm_ old but historic commentary here_dd_ _lt_a href_eq__qt_http_dd_//www.html5rocks.com/en/tutorials/getusermedia/intro/_qt_ rel_eq__qt_external nofollow_qt__gt_http_dd_//www.html5rocks.com/en/tutorials/getusermedia/intro/_lt_/a_gt__lt_/li_gt__lt_/ol_gt__lt_p_gt_Need to finish all other aspects_co_ then might circle back to this.  Only desktop I have with a microphone is MacBook Pro_co_ so hopefully can get something that is implemented there._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Deltakosh","Date":"2014-11-05T17:44:36Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt__lt_a href_eq__qt_https_dd_//status.modern.ie/webaudioapi?term_eq_web%20audio_qt_ rel_eq__qt_external nofollow_qt__gt_https_dd_//status.modern.ie/webaudioapi?term_eq_web%20audio_lt_/a_gt_ _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_smile.png_qt_ alt_eq__qt__dd_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/smile@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"gryff","Date":"2014-11-05T18:02:49Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_blockquote data-ipsquote_eq__qt__qt_ class_eq__qt_ipsQuote_qt_ data-ipsquote-contentcommentid_eq__qt_60032_qt_ data-ipsquote-contenttype_eq__qt_forums_qt_ data-ipsquote-contentclass_eq__qt_forums_Topic_qt_ data-ipsquote-contentid_eq__qt_10210_qt_ data-ipsquote-username_eq__qt_JCPalmer_qt_ data-cite_eq__qt_JCPalmer_qt_ data-ipsquote-timestamp_eq__qt_1415125691_qt__gt__lt_div_gt__lt_div_gt__lt_p_gt_Hey_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_My application is a game development tool_co_ not a game itself._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_a href_eq__qt_https_dd_//googledrive.com/host/0B6-s6ZjHyEwUSDVBUGpHOXdtbHc_qt_ rel_eq__qt_external nofollow_qt__gt_https_dd_//googledrive.com/host/0B6-s6ZjHyEwUSDVBUGpHOXdtbHc_lt_/a_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_The idea is to make game characters talk.  It is very difficult syncing meshes to audio.  Going from the other direction of syncing audio to meshes just means a little practice looking at the mesh in the tool.  Pre-recorded audio is also likely copyrighted_co_ so making your own audio is also free from issues._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_This tool does look ups of sentences in Carnegie Mellon University_t_s_co_ DARPA funded_co_ Phoneme database.  I wrote a Java program which converts it to a JavaScript module.  The database is big_co_ but you will only need it in the tools_co_ not the final application._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_It is not really done_co_ but getting there.  Will be adding a check box for starting a count down.  You could use a phone to make the actual recording_co_ but might be less fumble_co_ if the tool could do it._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Jeff_lt_/p_gt__lt_/div_gt__lt_/div_gt__lt_/blockquote_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Works really well Jeff _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_smile.png_qt_ alt_eq__qt__dd_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/smile@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I need to give him eyes and teeth - less of a zombie look _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_ohmy.png_qt_ alt_eq__qt__dd_o_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/ohmy@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_cheers_co_ gryff _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_smile.png_qt_ alt_eq__qt__dd_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/smile@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Deltakosh","Date":"2014-11-05T19:14:22Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hey this is cool!!!_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"joshcamas","Date":"2014-11-06T02:02:36Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Yea_co_ this is a really cool way of doing things - making speakings in-browser! Cool!_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2014-11-06T16:11:00Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_blockquote data-ipsquote_eq__qt__qt_ class_eq__qt_ipsQuote_qt_ data-ipsquote-contentcommentid_eq__qt_60223_qt_ data-ipsquote-contenttype_eq__qt_forums_qt_ data-ipsquote-contentclass_eq__qt_forums_Topic_qt_ data-ipsquote-contentid_eq__qt_10210_qt_ data-ipsquote-username_eq__qt_Deltakosh_qt_ data-cite_eq__qt_Deltakosh_qt_ data-ipsquote-timestamp_eq__qt_1415209476_qt__gt__lt_div_gt__lt_div_gt__lt_p_gt__lt_a href_eq__qt_https_dd_//status.modern.ie/webaudioapi?term_eq_web%20audio_qt_ rel_eq__qt_external nofollow_qt__gt_https_dd_//status.modern.ie/webaudioapi?term_eq_web%20audio_lt_/a_gt_ _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_smile.png_qt_ alt_eq__qt__dd_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/smile@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/p_gt__lt_/div_gt__lt_/div_gt__lt_/blockquote_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_DK_co_ Thanks for giving some info on the audio play / synthesizer  you are building upon.  Having a mockup_co_ maybe in d.ts form_co_ of how this is planned to be worked into BJS could mean I could work on implementing it before it was done._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_If I can successfully get this tool to record crisply timed audio files_co_ then I could maybe work their playback right into the same Voice.ts with just an optional arg.  Could even have 3 buttons in this tool_dd_  _qt_Say_qt__co_ _qt_Record_qt__co_ &amp_sm_ _qt_Playback_qt_._lt_/p_gt__lt_p_gt_- - - - - - - -_lt_/p_gt__lt_p_gt_Gryff_co_ a better head would be welcome.  Like every time I seem to get a new .blend with more features from you_co_ it shows up new problems.  Want this to work with a fully developed head that someone would actually use_co_ in development._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Also_co_ thinking that I am going to want a fourth shape key_co_ called _t_mouth-smile_t_.  Smiling does involve more than just the mouth_co_ but cheeks should be their own ShapeKeyGroup.  If you remember from the multi-group test (with the left_co_ right_co_ drumming_co_ &amp_sm_ conflict buttons)_co_ having different ShapeKeyGroups modifying the same vertices can cause problems._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"gryff","Date":"2014-11-06T16:42:01Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_blockquote data-ipsquote_eq__qt__qt_ class_eq__qt_ipsQuote_qt__gt__lt_div_gt_Also_co_ thinking that I am going to want a fourth shape key_co_ called _t_mouth-smile_t_.  Smiling does involve more than just the mouth_co_ but cheeks should be their own ShapeKeyGroup.  If you remember from the multi-group test (with the left_co_ right_co_ drumming_co_ &amp_sm_ conflict buttons)_co_ having different ShapeKeyGroups modifying the same vertices can cause problems._lt_/div_gt__lt_/blockquote_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Jeff_co_ the mouth-smile would involve cheeks as you state_co_ but it would also involve at least the corners of the mouth. Currently_co_ the mouth-wide is modifying vertices of the other two shape keys - so how big are the problems if the corners of the mouth were modified ? And if you want to go for modifying the corners of the mouth and cheeks - one or two keys (one for each side)?_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_As for improving the head - started that (added eyes this morning). Can I add additional bones to the armature (currently only a single bone)?_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_By the way_co_ I forgot to mention_co_ I liked the loudness effect and its impact on the shape keys._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_cheers_co_ gryff _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_smile.png_qt_ alt_eq__qt__dd_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/smile@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2014-11-06T17:40:35Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Gryff_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_There might be some combinations of mouth-smile &amp_sm_ mouth-wide that only the _qt_Joker_qt_ could pull off_co_ but it is not a code problem.  The additive shapekey concept of MORH.Mesh v 1.1 (formerly BABYLON.Automaton)_co_ means you just toss a ReferenceDeformation a 0 to 1 for each key you wish to combine.  It then builds a vertices endpoint that combines them all together &amp_sm_ computes matching endpoint normals_co_ or gets a precompiled set_co_ if it exists. _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Every beforeRender() call to MORPH.Mesh_co_ each shapekeygroup compares its new endpoint to the endpoint of the prior move_co_ and interpolates new vertex positions &amp_sm_ normals based on the amount of time that has elapsed over how long you said it was supposed to take.  No pre-compiled _qt_frames_qt_.  That just leads to choppyiness_co_ kind of like in math of doing rounding before the last step._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_The problem with vertices across Shapekeygroups is it cannot be additive like intra-group keys.  It is last group run wins._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_One key for smile &amp_sm_ one bone is fine. _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_The version of the tool in the link is already old_co_ &amp_sm_ does not pre-compile endpoints.  Doing this makes it even smoother_co_ especially for quick moves._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Thought you would like loudness.  The loudest is 100% of the values for the keys you provided.  Looks  un natural.  Thinking about dropping it_co_ as the # pre-compiles is based on the total # of shapes it could take.  Currently 10 VISEMES * 3 loudnesses.  Add in a 2 smile settings_co_ and that would be 60.  They are small_co_ but if no-one would use the loudest_co_ could drop it &amp_sm_ only have 40._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Jeff_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2014-11-19T22:11:09Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_blockquote data-ipsquote_eq__qt__qt_ class_eq__qt_ipsQuote_qt_ data-ipsquote-contentcommentid_eq__qt_60018_qt_ data-ipsquote-contenttype_eq__qt_forums_qt_ data-ipsquote-contentclass_eq__qt_forums_Topic_qt_ data-ipsquote-contentid_eq__qt_10210_qt_ data-ipsquote-username_eq__qt_davrous_qt_ data-cite_eq__qt_davrous_qt_ data-ipsquote-timestamp_eq__qt_1415118778_qt__gt__lt_div_gt__lt_p_gt__lt_/p_gt__lt_div_gt__lt_p_gt__lt_/p_gt__lt_p_gt_Hi_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ I_t_m in charge of the Web audio engine. I_t_ll focus myself on the sound playback. Recording needs the usage of the Media Capture API. Why would you need sound recording in a game?_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Bye_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_David_lt_/p_gt__lt_/div_gt__lt_/div_gt__lt_/blockquote_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Though all these standards are conflicting_co_ I did manage to get the Web Audio API to record as well.  Updated the link above.  Ignore the Mood slider.  It is not implemented yet.  Did not do a countdown.  Just recording over an over is faster.  Use Playback button to verify result. _qt_.wav_qt_ button puts a file in your downloads directory._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Not sure on the timing of your implementation of Web Audio in 2.0.  Am going with my own at least for recording.  I do not know how to read back in the saved .wav file.  Here is my AudioRecorder.ts file_dd__lt_/p_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_module MORPH {    /** has its origins from_dd_  _lt_a href_eq__qt_http_dd_//bytearray.org/wp-content/projects/WebAudioRecorder/_qt__gt_http_dd_//bytearray.org/wp-content/projects/WebAudioRecorder/_lt_/a_gt_ */    export class AudioRecorder{        private context _dd_ AudioContext_sm_                public initialized _eq_ false_sm_  // set in prepMic_sm_ will remain false if WebAudio or navigator.getUserMedia not supported        public playbackReady _eq_ false_sm_        private recording _eq_ false_sm_        private requestedDuration _dd_ number_sm_        private startTime _dd_ number_sm_        private recordCompletionCallback _dd_ () _eq_&gt_sm_ void_sm_                        // arrays of FloatArrays made during recording        private leftchannel  _eq_ new Array&lt_sm_Float32Array&gt_sm_()_sm_        private rightchannel _eq_ new Array&lt_sm_Float32Array&gt_sm_()_sm_        // consolidated versions of the buffer_co_ after recording for playback or written to .WAV        private leftBuffer  _dd_ Float32Array_sm_        private rightBuffer _dd_ Float32Array_sm_        private recorder _dd_ ScriptProcessorNode _eq_ null_sm_        private recordingLength _eq_ 0_sm_        private volume _dd_ GainNode _eq_ null_sm_        private audioInput _eq_ null_sm_                private objectUrl _dd_ string_sm_                private static instance _dd_ AudioRecorder_sm_        public static getInstance() _dd_ AudioRecorder{            if (AudioRecorder.instance) return AudioRecorder.instance_sm_                        AudioRecorder.instance _eq_ new AudioRecorder()_sm_                        var audioContext _eq_ window.AudioContext || window.webkitAudioContext_sm_            if (audioContext) {                window.alert(_t_Asking for audio recording permission in advance_co_\\nto avoid asking when actually trying to record._t_)_sm_                            navigator.getUserMedia _eq_ navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia_sm_                if (navigator.getUserMedia){                    navigator.getUserMedia({audio_dd_true}_co_ AudioRecorder.prepMic_co_ function(stream _dd_ any) { window.alert(_t_Error capturing audio._t_ + stream)_sm_ })_sm_                                     } else {                    window.alert(_t_Navigator.getUserMedia not supported._t_)_sm_                 }                            }else{                window.alert(_t_WebAudio not supported_t_)_sm_            }                          return AudioRecorder.instance_sm_        }                /**         *  static because it is in a callback for navigator.getUserMedia()         */        private static prepMic(stream _dd_ any){            AudioRecorder.instance.context _eq_ new (window.AudioContext || window.webkitAudioContext)()_sm_            AudioRecorder.instance.context.sampleRate _eq_ 44100_sm_                        // creates a gain node            AudioRecorder.instance.volume _eq_ AudioRecorder.instance.context.createGain()_sm_            // creates an audio node from the microphone incoming stream            AudioRecorder.instance.audioInput _eq_ AudioRecorder.instance.context.createMediaStreamSource(stream)_sm_            // connect the stream to the gain node            AudioRecorder.instance.audioInput.connect(AudioRecorder.instance.volume)_sm_            /* From the spec_dd_ This value controls how frequently the audioprocess event is             dispatched and how many sample-frames need to be processed each call.             Lower values for buffer size will result in a lower (better) latency.             Higher values will be necessary to avoid audio breakup and glitches */            var bufferSize _eq_ 2048_sm_            AudioRecorder.instance.recorder _eq_ AudioRecorder.instance.context.createScriptProcessor(bufferSize_co_ 2_co_ 2)_sm_            // cannot reference using _t_this_t_ inside of callback            AudioRecorder.instance.recorder.onaudioprocess _eq_ function(e){                if (!AudioRecorder.instance.recording) return_sm_                var left _eq_ e.inputBuffer.getChannelData (0)_sm_                var right _eq_ e.inputBuffer.getChannelData (1)_sm_                // we clone the samples                AudioRecorder.instance.leftchannel.push (new Float32Array (left))_sm_                AudioRecorder.instance.rightchannel.push (new Float32Array (right))_sm_                AudioRecorder.instance.recordingLength +_eq_ bufferSize_sm_                // determine if the duration required has yet occurred                if (Mesh.now() - AudioRecorder.instance.requestedDuration &gt_sm__eq_ AudioRecorder.instance.startTime) AudioRecorder.instance.recordStop()_sm_            }_sm_            // we connect the recorder            AudioRecorder.instance.volume.connect (AudioRecorder.instance.recorder)_sm_            AudioRecorder.instance.recorder.connect (AudioRecorder.instance.context.destination)_sm_             AudioRecorder.instance.initialized _eq_ true_sm_        }                public recordStart(durationMS _dd_ number_co_ doneCallback? _dd_ () _eq_&gt_sm_ void){            if (this.recording){ BABYLON.Tools.Warn(_qt_already recording_qt_)_sm_ return_sm_ }            this.recording _eq_ true_sm_            this.requestedDuration _eq_ durationMS_sm_            this.startTime _eq_ Mesh.now()_sm_            this.recordCompletionCallback _eq_ doneCallback ? doneCallback _dd_ null_sm_                        // delete previous merged buffers_co_ if they exist            this.leftBuffer _eq_ this.rightBuffer _eq_ null_sm_            this.playbackReady _eq_ false_sm_        }        public recordStop() _dd_ void{            if (!this.recording) {BABYLON.Tools.Warn(_qt_recordStop when not recording_qt_)_sm_ return_sm_ }            this.recording _eq_ false_sm_            // we flatten the left and right channels down            this.leftBuffer  _eq_ this.mergeBuffers (this.leftchannel )_sm_            this.rightBuffer _eq_ this.mergeBuffers (this.rightchannel)_sm_            this.playbackReady _eq_ true_sm_                        this.clean()_sm_            if (this.recordCompletionCallback) this.recordCompletionCallback()_sm_        }        public playback() _dd_ void {            if (!this.playbackReady) {BABYLON.Tools.Warn(_qt_playback when not playbackReady_qt_)_sm_ return_sm_ }            var newSource _eq_ this.context.createBufferSource()_sm_            var newBuffer _eq_ this.context.createBuffer( 2_co_ this.leftBuffer.length_co_ this.context.sampleRate )_sm_                        newBuffer.getChannelData(0).set(this.leftBuffer)_sm_            newBuffer.getChannelData(1).set(this.rightBuffer)_sm_            newSource.buffer _eq_ newBuffer_sm_            newSource.connect( this.context.destination )_sm_            newSource.start(0)_sm_        }        public saveToWAV(filename _dd_ string) _dd_ void{            if (!this.playbackReady) {BABYLON.Tools.Warn(_qt_save when not playbackReady_qt_)_sm_ return_sm_ }                        if (filename.length _eq__eq__eq_ 0){                window.alert(_qt_No name specified_qt_)_sm_                return_sm_            }            else if (filename.toLowerCase().lastIndexOf(_qt_.wav_qt_) !_eq__eq_ filename.length - 4){                filename +_eq_ _qt_.wav_qt__sm_            }                        var blob _eq_ new Blob ( [ this.encodeWAV() ]_co_ { type _dd_ _t_audio/wav_t_ } )_sm_                    // turn blob into an object URL_sm_ saved as a member_co_ so can be cleaned out later             this.objectUrl _eq_ (window.webkitURL || window.URL).createObjectURL(blob)_sm_                        var link _eq_ window.document.createElement(_t_a_t_)_sm_            link.href _eq_ this.objectUrl_sm_            link.download _eq_ filename_sm_            var click _eq_ document.createEvent(_qt_MouseEvents_qt_)_sm_            click.initEvent(_qt_click_qt__co_ true_co_ false)_sm_            link.dispatchEvent(click)_sm_                    }                private clean() _dd_ void {            if (this.objectUrl){                (window.webkitURL || window.URL).revokeObjectURL(this.objectUrl)_sm_                this.objectUrl _eq_ null_sm_            }                        // reset the buffers for the new recording            this.leftchannel.length _eq_ this.rightchannel.length _eq_ 0_sm_            this.recordingLength _eq_ 0_sm_        }                    private mergeBuffers(channelBuffer _dd_ Array&lt_sm_Float32Array&gt_sm_) _dd_ Float32Array{            var result _eq_ new Float32Array(this.recordingLength)_sm_            var offset _eq_ 0_sm_            var lng _eq_ channelBuffer.length_sm_            for (var i _eq_ 0_sm_ i &lt_sm_ lng_sm_ i++){                var buffer _eq_ channelBuffer[i]_sm_                result.set(buffer_co_ offset)_sm_                offset +_eq_ buffer.length_sm_            }            return result_sm_        }                private interleave() _dd_ Float32Array{            var length _eq_ this.leftBuffer.length + this.rightBuffer.length_sm_            var result _eq_ new Float32Array(length)_sm_            var inputIndex _eq_ 0_sm_            for (var index _eq_ 0_sm_ index &lt_sm_ length_sm_ ){                result[index++] _eq_ this.leftBuffer[inputIndex]_sm_                result[index++] _eq_ this.rightBuffer[inputIndex]_sm_                inputIndex++_sm_            }            return result_sm_        }                private encodeWAV() _dd_ DataView{            // we interleave both channels together            var interleaved _eq_ this.interleave()_sm_            var buffer _eq_ new ArrayBuffer(44 + interleaved.length * 2)_sm_            var view _eq_ new DataView(buffer)_sm_                    // RIFF chunk descriptor            this.writeUTFBytes(view_co_ 0_co_ _t_RIFF_t_)_sm_            view.setUint32(4_co_ 44 + interleaved.length * 2_co_ true)_sm_            this.writeUTFBytes(view_co_ 8_co_ _t_WAVE_t_)_sm_            // FMT sub-chunk            this.writeUTFBytes(view_co_ 12_co_ _t_fmt _t_)_sm_            view.setUint32(16_co_ 16_co_ true)_sm_            view.setUint16(20_co_ 1_co_ true)_sm_            // stereo (2 channels)            view.setUint16(22_co_ 2_co_ true)_sm_            view.setUint32(24_co_ this.context.sampleRate_co_ true)_sm_            view.setUint32(28_co_ this.context.sampleRate * 4_co_ true)_sm_            view.setUint16(32_co_ 4_co_ true)_sm_            view.setUint16(34_co_ 16_co_ true)_sm_            // data sub-chunk            this.writeUTFBytes(view_co_ 36_co_ _t_data_t_)_sm_            view.setUint32(40_co_ interleaved.length * 2_co_ true)_sm_                    // write the PCM samples            var lng _eq_ interleaved.length_sm_            var index _eq_ 44_sm_            var volume _eq_ 1_sm_            for (var i _eq_ 0_sm_ i &lt_sm_ lng_sm_ i++){                view.setInt16(index_co_ interleaved[i] * (0x7FFF * volume)_co_ true)_sm_                index +_eq_ 2_sm_            }            return view_sm_        }        private writeUTFBytes(view_co_ offset_co_ string){             var lng _eq_ string.length_sm_            for (var i _eq_ 0_sm_ i &lt_sm_ lng_sm_ i++){                view.setUint8(offset + i_co_ string.charCodeAt(i))_sm_            }        }    }}_lt_/pre_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2014-11-20T00:04:30Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi Jeff_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Impressive. _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_wink.png_qt_ alt_eq__qt__sm_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/wink@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_What you are trying to build is a huge leap towards a valuable tool.  With more than 15 years of experience in feature film and game facial animation such as The Hobbit and GTA IV_co_ I would tackle this as two different tasks that have to be separate functions_co_ and possibly separate applications initially - as they are very different from each other.  For speech_co_ if you narrow the available phenoms to approximately a dozen_co_ you will be able to provide reliable quality speech and can focus on a feature set / API that is robust.  Then to begin with_co_ there are a few methods to speech aside from keyframing_co_ but video analysis and speech recognition are the most reliable in my opinion.  And_co_ since it took us years to perfect video analysis at Weta for Avatar_co_ my vote is speech recognition driving morph targets and at least one bone driving the jaw (or morph target - a bone provides better results and can also be used in the facial expressions for both vertical and lateral movement.)  This bone or morph target is used to to attenuate/amplify any phoneme._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_For the voice recognition_co_ take a look at the source for Google_t_s speech recognition tool_dd__lt_/p_gt__lt_p_gt__lt_a href_eq__qt_https_dd_//github.com/GoogleChrome/webplatform-samples/tree/master/webspeechdemo_qt_ rel_eq__qt_external nofollow_qt__gt_https_dd_//github.com/GoogleChrome/webplatform-samples/tree/master/webspeechdemo_lt_/a_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I can_t_t imagine that anyone needs all of the function of their Webspeech API_co_ as more than 12 - 16 phenoms for real time or post analysis will produce very choppy results when applied._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Facial is far more difficult as there are primarily 4 viable methods to achieve this_dd__lt_/p_gt__lt_p_gt_1. Keyframing (time consuming and choppy)_lt_/p_gt__lt_p_gt_2. Audio analysis (poor results)_lt_/p_gt__lt_p_gt_3. Puppeteering (real time)_lt_/p_gt__lt_p_gt_4. Facial recognition (video analysis - currently prohibitive)_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_For many of the Trolls in The Hobbit_co_ I built a real time digital and animatronic puppeting application - which took me approximately a week to begin to test in production.  Even all the way back to Nickelodeon_t_s CatDog_co_ I found the simplest path to good facial was to puppeteer their faces in real time.  This is as simple as connecting a joystick controller to morph targets on a mesh_co_ and to set up conditional blending between morph targets - which is really basic math._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_My goal is also to create a function in babylon.js to achieve quality facial animation with minimal effort.  At this time_co_ I_t_m tasked with production deliveries_co_ and am still stumbling through the tiny (and not so tiny) pitfalls of a new and unforgiving development framework (only a couple of weeks in.)  I chose babylon.js over three.js for many_co_ many reasons_co_ and am now certain it is the most straight forward and flexible framework available.  And as my co-partner/developer owns patents for multimedia streaming_co_ we already have compiled streaming applications that we are in process of adapting to WebGL.  So our plate is _qt_currently_qt_ full.  However_co_ with years of experience in facial_co_ I can provide a list of a dozen or so key phonemes that are essential to good speech_co_ as well as key morph targets for facial puppeteering.  It could be a fun project.  _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_smile.png_qt_ alt_eq__qt__dd_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/smile@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/p_gt__lt_p_gt_A tongue. eyeballs_co_ and eyelids will also be required eventually to improve speech and facial to a believable threshold.  If I can assist in any way_co_ please let me know._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Cheers_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_David B._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2014-11-20T16:29:44Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_David B._lt_/p_gt__lt_p_gt_Thanks for responding.  I am doing this for a commercial_co_ interactive_co_ entertainment application.  Call it a _qt_game_qt_.  If you are not sure what to make of that_dd_  Mission Accomplished _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_smile.png_qt_ alt_eq__qt__dd_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/smile@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt_.  Saw a news report back on 4/1 about Facebook buying Occulus_co_ and an idea popped into my head.  I had not even heard of Occulus.  My app has nothing to do with Occulus Rift_co_ but it was part of my early searches that pointed me to BJS._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_This MORPH package is just one of the parts I am going to need.  With Gryff_t_s pointing out of speech / shape keys_co_ and helping me tremendously_co_ I was able to get this far.  I have adapted my plans to also make characters talk.  Should point out_co_ if you did not know_co_ Meshes do not even need to be humans_co_ much less talk_co_ to use this morphing capability._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_My background is heavily in Java / SQL &amp_sm_ some OpenCL in the domain of finance / investing.  I have approached speaking like a couple of informal databases.  My database skills are quite good (It took less than 1 day to write a Java program to take a Arpabet flat webpage to a Javascript Module with a look up by index).  I also distinguish between sounds &amp_sm_ mouth shapes (phonemes &amp_sm_ vismes).  Currently_co_ there is a 39 row Arpabet phoneme _qt_table_qt_ which references a 11 row viseme _qt_table_qt_._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_MOPRH.Voice.makeSentence() takes an Arpabet token string.  It looks up each token in the phoneme table to get an index into the viseme table as well as the duration of the phoneme.  The viseme table has shapekey settings to use for each viseme.  The values for the keys are combined  to derive a final morph target._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Some of the table values need to be refined_co_ but others seem to be dead on.  If you have data on visemes or how they should be mapped to Arpabet phonemes that would be great!_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_The big development you should know about concerns what MakeHuman is doing.  Apparently_co_ they had Blender viseme shapekeys you could import.  They are close to coming out with a new version where this is supposed to be greatly improved.  The number of keys they are using were different from Gryff.  I am not good enough in Blender to make my own characters or good shapekeys.  I hope to do a database restructure to support this instead when it comes out.  You might want to get familiar with MakeHuman_co_ if you are not._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Kind on hold on this for now.  Going to work on a couple of bugs with the Blender exporter_co_ and publish v1.1 as is.  V1.1 works with BJS 1.14_co_ but V1.2 is going to work with BJS 2.0. (FYI_co_ I am not going to clog up the repository with the Arpabet module_co_ cmudict.0.7.a.js_co_ but will include the Java source / class file which builds it)._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Jeff_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2014-11-20T20:53:09Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi Jeff_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Wow_co_ I thought I was one of the only people who knew what arpabet phonemes were. _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_smile.png_qt_ alt_eq__qt__dd_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/smile@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt_  Almost all of the work we have done in voice recognition utilizes arpabet phonemes.  I took a look at the UC Berkely study you posted_co_ and was concerned that it might provide too much info for anyone trying to animate speech.  I can see you have a good grasp of how to generate reasonable animated speech - far more than most people I_t_ve had the pleasure of working with.  It_t_s no easy task_co_ and you_t_ve the first person I_t_ve found to make a serious step forward to try and create a framework for analysis/mapping of speech in WebGL._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_The main thought I had was what most people don_t_t fully understand animated speech until they_t_ve done more work than necessary - i.e. cmudict.0.7.a.js contains far more arpabet phonemes than animators would ever use_co_ as most people tend to want to use too many of these as morph targets.  However_co_ it_t_s good to reference these in audio recognition_co_ and to pair them down to a third or less morph targets to be driven._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I come from a development background that couldn_t_t be further away from HTML 5 / WebGL_co_ so I_t_m a newbie in the community.  On this board_co_ you_t_ve been someone who has pointed me in the right direction as I_t_m going through the birth pains. I_t_m currently working in Blender to accelerate my learning curve in WebGL_co_ but I prefer Maya for most of my modelling_co_ rigging_co_ etc..  I have recently discovered MakeHuman_co_ and it_t_s certainly interesting.  Much of the work I_t_ve done the past few years for software plugins is in python - too many languages_co_ too little time. _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_wacko.png_qt_ alt_eq__qt__dd_wacko_dd__qt__gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I_t_m not certain if I can assist_co_ but if you have a mesh and need morph targets_co_ let me know and I can generate these for you to use - providing I better understand the design/constraints of your analysis process_co_ and your vision for a result/output.  This is of extreme interest to me_co_ and of course would be a valuable tool for the WebGL community._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Cheers_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_David B._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2014-11-20T22:13:55Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_David_co__lt_/p_gt__lt_p_gt_I listed my _qt_contributors_qt__co_ like the Berkley study_co_ only to give them credit.  The user has no operational need to know this.  I grabbed the duration setting for each row in the Arpabet phoneme table from there._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_As far the cmudict.0.7.a.js is concerned_co_ you probably know this_co_ but that big file only needs to be used in the dev tool itself_co_ not in any game.  The outputs of this tool are tiny Arpabet sentences (Tool does not actually allow you to put them on the clipboard yet)_co_ and your .wav files.  The tool shows the intermediate Arpabet_co_ precisely because it is an output that needs to be copied to the game. _lt_strong_gt_ As far as a list of Arpabet phonemes that I should ignore_co_ I am all ears!_lt_/strong_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_There is one nice consequence_co_ for the low cost / hobby developer_co_ of being a html 5 page.  You could email someone who would be willing to make recordings for you a list of sentences &amp_sm_ speeds_co_ and a link to the tool.  They could just email the .wav files back._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_The slider controls_dd_  loudness &amp_sm_ speed are just settings you need to remember_co_ so you can set them to the same value in the game_co_ with your actual mesh._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_You are going to like that MakeHuman &amp_sm_ Blender are extensively written in Python.  MakeHuman does not look like it handles third party plugins_co_ but Blender does.  MakeHuman has a Blender importer &amp_sm_ there 2 variations of a blender exporter from Blender. _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_If you are referring to javascript coding_co_ I you might be better doing any coding in typescript &amp_sm_ compiling to javascript.  It is much closer to python than javascript (Javascript is so screwed up in my opinion).  One issue is all sample code / tutorials seems to be written in Javascript though.  The Blender exporter variant I have in the Extensions repository actually outputs generated .js / or .ts source code files_co_ not a .babylon._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Jeff_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2014-11-20T23:11:43Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi Jeff_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Here is a list of fundamental  face states and phonemes I use for audio analysis and real time animation_dd__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_u_gt_Fundamental Facial / Mouth States_lt_/u_gt__lt_/p_gt__lt_p_gt_Default_lt_/p_gt__lt_p_gt_Silence_lt_/p_gt__lt_p_gt_Breath_lt_/p_gt__lt_p_gt_Loud/Shout_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_u_gt_Phonemes_lt_/u_gt__lt_/p_gt__lt_p_gt_AE_lt_/p_gt__lt_p_gt_AO_lt_/p_gt__lt_p_gt_AX_lt_/p_gt__lt_p_gt_E_lt_/p_gt__lt_p_gt_FV_lt_/p_gt__lt_p_gt_H_lt_/p_gt__lt_p_gt_IY_lt_/p_gt__lt_p_gt_KG_lt_/p_gt__lt_p_gt_L_lt_/p_gt__lt_p_gt_M_lt_/p_gt__lt_p_gt_N_lt_/p_gt__lt_p_gt_OW_lt_/p_gt__lt_p_gt_PB_lt_/p_gt__lt_p_gt_SZ_lt_/p_gt__lt_p_gt_TD_lt_/p_gt__lt_p_gt_UH_lt_/p_gt__lt_p_gt_UW_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_This can be overkill_co_ depending on the characteristics of the .wav file you_t_re working with.  Every recording device and every voice has different characteristics.  I never run .wav analysis without a waveform equalizer in line.  If the voice is recorded on a quality recording device (20hz - 18hz+)_co_ then you will want to push the frequencies from 1khz+ to (3khz and 5khz)_co_ depending on the result.  A 3 band parametric EQ works the best.  As audio recording varies so dramatically by device and by person_co_ a 20hz to 20khz 3 band parametric EQ is the best - I was an audio engineer prior to wearing my pseudo developer_t_s hat._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Another very key element for quality facial from waveform analysis has been to provide the user with an attenuation / amplification slider per Facial State / Phoneme_co_ as the quality of facial animation generally comes from individual phoneme attenuation / amplification.  It_t_s not enough to recognize a phoneme_co_ but to then adjust for audio quality_co_ dialect_co_ idiosyncrasies in speech_co_ etc._co_ and this will dramatically affect output.  Adding these controls will improve animation quality by immeasurable amounts._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_It took me a long time to perfect this for film_co_ television_co_ and games_co_ as I went into it with complete ignorance.  You_t_re way ahead of the curve and truly onto something._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I really appreciate you introducing me to Typescript - it_t_s just what I_t_ve been looking for.  It seems simple enough to use_co_ and I prefer to work in Visual Studio - depending on it_t_s integration._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Cheers_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_David B._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_strong_gt_P.S. I made some corrections to this post_co_ as I copy a bit of my posts from docs as well as use a text editor that will spell words based on the text in play_co_ and spelled phonemes wrong for most of this post_lt_/strong_gt_. _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_blink.png_qt_ alt_eq__qt__dd_blink_dd__qt__gt__lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2014-11-25T15:29:23Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_David_co__lt_/p_gt__lt_p_gt_Thanks! Am beginning to work in the ignores.  Your statement that I was kind of doing multiple things at the same time from your first post has been marinating. I am in the process of addressing this and will be publishing a new version soon.  It will only be a start.  I do not expect to nail it the first time.  Stay tuned._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Your recording hints were interesting_co_ but it would be really good if you could elaborate on _qt_quality recording_qt_ from a computer sound card perspective.  Things like_dd__lt_/p_gt__lt_ul_gt__lt_li_gt_Using the sound card jacks vs through the computer via USB.  I bought a cheap sound jack headset_co_ since I am running Linux.  The medium priced were USB_co_ but no Linux support.  High priced were USB &amp_sm_ had Linux drivers_co_ but is this really a better Mic_co_ or is all the extra in the headphones part?_lt_/li_gt__lt_li_gt_Are all sound boards the same?  Are there some that are worth upgrading to for recording capabilities?  Are these all just commodity components now days?_lt_/li_gt__lt_li_gt_PC vs Mac recording.  Any difference?  I have both._lt_/li_gt__lt_li_gt_Stereo vs mono.  My code writes stereo_co_ but think it is worth the effort to make this switchable._lt_/li_gt__lt_/ul_gt__lt_p_gt_Jeff_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2014-11-25T16:02:45Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Am in the process of turning phonemes off that are not on the list.  What if the word is _t_hot_t_?  The result is _t_HH_t_ _t_AA_t_ _t_T_t_.  None is on your list_co_ so no change would happen.  What were you doing in this case?_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Here is my phoneme table.  The first argument of the Phonme constructor is an index into the VISMES table_co_ the sencond is an index into an array used for duration.  The higher the index_co_ the shorter the duration.  Comments to the right with examples._lt_/p_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_        private static ARPABET_DICT _eq_ {            _qt_._qt_ _dd_ new Phoneme(10_co_ 0)_co_ // rest_sm_ durationIdx ignored_co_ SPEECH_RATE used            _qt_AA_qt__dd_ new Phoneme( 0_co_ 1)_co_ // VOWEL_dd_ hOt_co_ wAnt_co_ bOUGHt_co_ Odd            _qt_AE_qt__dd_ new Phoneme( 0_co_ 1)_co_ // VOWEL_dd_ At_co_             _qt_AH_qt__dd_ new Phoneme( 0_co_ 4)_co_ // VOWEL_dd_ Up_co_ Alone_co_ hUt            _qt_AO_qt__dd_ new Phoneme( 2_co_ 1)_co_ // VOWEL_dd_ Off_co_ fAll_co_ frOst_co_ hAUl_co_ drAW            _qt_AW_qt__dd_ new Phoneme( 2_co_ 0)_co_ // VOWEL_dd_ cOW_co_ OUt_co_ mOUsE_co_ hOUsE            _qt_AY_qt__dd_ new Phoneme( 0_co_ 1)_co_ // VOWEL_dd_ fInd_co_ rIdE_co_ lIGHt_co_ flY_co_ pIE            _qt_B_qt_ _dd_ new Phoneme( 7_co_ 2)_co_ // CONS _dd_ Big_co_ ruBBer            _qt_CH_qt__dd_ new Phoneme( 9_co_ 1)_co_ // CONS _dd_ CHip_co_ maTCH            _qt_D_qt_ _dd_ new Phoneme( 4_co_ 3)_co_ // CONS _dd_ Dog_co_ aDD_co_ fillED            _qt_DH_qt__dd_ new Phoneme( 6_co_ 4)_co_ // CONS _dd_ THis_co_ feaTHer_co_ THen            _qt_EH_qt__dd_ new Phoneme( 0_co_ 2)_co_ // VOWEL_dd_ bEd_co_ brEAd            _qt_ER_qt__dd_ new Phoneme( 3_co_ 1)_co_ // VOWEL_dd_ bURn_co_ fIRst_co_ fERn_co_ hEARd_co_ wORk_co_ dollAR            _qt_EY_qt__dd_ new Phoneme( 0_co_ 1)_co_ // VOWEL_dd_ bAcon_co_ lAtE_co_ dAY_co_ trAIn_co_ thEY_co_ EIght_co_ vEIn            _qt_F_qt_ _dd_ new Phoneme( 5_co_ 2)_co_ // CONS _dd_ Fish_co_ PHone            _qt_G_qt_ _dd_ new Phoneme( 4_co_ 3)_co_ // CONS _dd_ Go_co_ eGG            _qt_HH_qt__dd_ new Phoneme( 0_co_ 4)_co_ // CONS _dd_ Hot_co_ House            _qt_IH_qt__dd_ new Phoneme( 0_co_ 2)_co_ // VOWEL_dd_ mIRRor_co_ chEER_co_ nEAR_co_ If_co_ bIg_co_ wIn            _qt_IY_qt__dd_ new Phoneme( 1_co_ 1)_co_ // VOWEL_dd_ shE_co_ thEsE_co_ bEAt_co_ fEEt_co_ kEY_co_ chIEf_co_ babY            _qt_JH_qt__dd_ new Phoneme( 9_co_ 1)_co_ // CONS _dd_ Jet _co_caGe_co_ barGe_co_ juDGE_co_ Gym            _qt_K_qt_ _dd_ new Phoneme( 4_co_ 2)_co_ // CONS _dd_ Cat_co_ Kitten_co_ duCK_co_ sCHool_co_ oCCur            _qt_L_qt_ _dd_ new Phoneme( 6_co_ 2)_co_ // CONS _dd_ Leg_co_ beLL            _qt_M_qt_ _dd_ new Phoneme( 7_co_ 2)_co_ // CONS _dd_ Mad_co_ haMMer_co_ laMB            _qt_N_qt_ _dd_ new Phoneme( 4_co_ 4)_co_ // CONS _dd_ No_co_ diNNer_co_ KNee_co_ GNome            _qt_NG_qt__dd_ new Phoneme( 4_co_ 1)_co_ // CONS _dd_ siNG_co_ moNkey_co_ siNk            _qt_OW_qt__dd_ new Phoneme( 2_co_ 1)_co_ // VOWEL_dd_ nO_co_ nOtE_co_ bOAt_co_ sOUl_co_ rOW            _qt_OY_qt__dd_ new Phoneme( 2_co_ 1)_co_ // VOWEL_dd_ cOIn_co_ tOY            _qt_P_qt_ _dd_ new Phoneme( 7_co_ 4)_co_ // CONS _dd_ Pie_co_ aPPle            _qt_R_qt_ _dd_ new Phoneme( 4_co_ 3)_co_ // CONS _dd_ Run_co_ maRRy_co_ WRite            _qt_S_qt_ _dd_ new Phoneme( 4_co_ 2)_co_ // CONS _dd_ Sun_co_ mouSE_co_ dreSS_co_ City_co_ iCE_co_ SCienCE            _qt_SH_qt__dd_ new Phoneme( 9_co_ 2)_co_ // CONS _dd_ SHip_co_ miSSion_co_ CHef_co_ moTIon_co_ speCIal            _qt_T_qt_ _dd_ new Phoneme( 4_co_ 4)_co_ // CONS _dd_ Top_co_ leTTer_co_ sToppED            _qt_TH_qt__dd_ new Phoneme( 4_co_ 3)_co_ // CONS _dd_ THumb_co_ THin_co_ THing            _qt_UH_qt__dd_ new Phoneme( 2_co_ 3)_co_ // VOWEL_dd_ bOOk_co_ pUt_co_ cOULd            _qt_UW_qt__dd_ new Phoneme( 2_co_ 2)_co_ // VOWEL_dd_ hUman_co_ UsE_co_ fEW_co_ tWO            _qt_V_qt_ _dd_ new Phoneme( 5_co_ 4)_co_ // CONS _dd_ Vet_co_ giVe            _qt_W_qt_ _dd_ new Phoneme( 8_co_ 3)_co_ // CONS _dd_ Wet_co_ Win_co_ sWim_co_ WHat            _qt_Y_qt_ _dd_ new Phoneme( 1_co_ 2)_co_ // CONS _dd_ Yes _co_ onIon            _qt_Z_qt_ _dd_ new Phoneme( 4_co_ 2)_co_ // CONS _dd_ Zip_co_ fiZZ_co_ sneeZE_co_ laSer_co_ iS_co_ waS_co_ pleaSE_co_ Xerox_co_ Xylophone            _qt_ZH_qt__dd_ new Phoneme( 9_co_ 1)_co_ // CONS _dd_ garaGE_co_ meaSure_co_ diviSion        }_sm__lt_/pre_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2014-11-26T21:36:46Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi Jeff_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_My apologies_co_ as I_t_ve been slammed with work before the holiday.  REALLY looking forward to Thanksgiving.  _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_biggrin.png_qt_ alt_eq__qt__dd_D_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/biggrin@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt_  There is no advantage to using USB over analogue microphones or inputs_co_ as every microphone is analogue_co_ and USB is simply a conversion to digital.  The analogue mic will be converted to a digital signal in any computer audio board_co_ and all audio boards now are 32 bit and 64 bit - which for speech_co_ 32 bit is just as good.  _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_As for microphones_co_ the rule is that the size of the microphone (actually the diaphragm inside the microphone) is relative to audio quality_co_ however for human speech_co_ a headset microphone from Audiotechnica or Plantronics is fine.  I_t_ve used $300 Sennheiser headsets_co_ and have found nominal improvement to frequency response in recording human speech - so a microphone in the $50 range plugged directly into the audio jack of almost any modern computer is very good for this application.  I would use a headset microphone without headphones (as headphones only add to the cost)_co_ and not a stationary microphone so that you don_t_t have to be concerned about directional attributes.  If you use the stationary microphone in a laptop_co_ the audio quality is reduced and you_t_ll see a dramatic decrease in audio quality - which cannot be improved much._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_One key piece of external hardware that I find important in a portable audio rig is a small mixer for audio quality control as well as other features.  I use a Mackie 402VLZ4 4 channel mixer_co_ as it provides 48 volt phantom power to some microphones which require this.  So if you_t_re shopping for microphones_co_ make sure you check to see if they require 48 volt phantom power which will not work without a mixer which supports 48 volt phantom power.  Phantom power is a bonus_co_ as the audio signal is consistently strong and simply a better microphone.  These usually run in the neighborhood of $100 for entry level which is fine for your application.  I would like to use a mixer such as the Behringer 802 mixer as it provides a single band of parametric EQ in addition to the low band and high band analogue EQs_co_ but does not provide Phantom power - so it_t_s a balance of budget and features.  I use an outboard digital parametric EQ_co_ but these are quite expensive. _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_sad.png_qt_ alt_eq__qt__dd_(_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/sad@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt_  _lt_/p_gt__lt_p_gt_The Mackie mixer I use will cost about $99_co_ and the Behringer costs about $65.  Also_co_ if you do place a mixer in-line_co_ don_t_t forget to order a 1/8_qt_ adapter for your microphone (some come with this) and a cable with a mono _qt_1/4_qt_ jack on one end and a mono or stereo 1/8_qt_ jack on the other end.  Be aware that if you are using a mono input_co_ the signal will be missing from one of the stereo channels your computer audio board is processing - which is fine if you_t_re aware.  I_t_m sure this is remedial info_co_ but I didn_t_t want to skip anything._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I hope this helps_co_ as audio signal quality and versatility is the most important element in the system_co_ as everything is on a level playing field once the signal is converted by your audio board._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Cheers_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_David B._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2014-11-27T03:26:46Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi Jeff_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I wrote a reply to this today which seems to have been lost.  _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_sad.png_qt_ alt_eq__qt__dd_(_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/sad@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt_  I_t_ll do my best to repeat the info.  First_co_ my apologies for not responding sooner_co_ but I_t_ve been in overdrive trying to produce work before the holidays._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Here_t_s what I believe I posted earlier today - I should have used my text editor - which I_t_m still ignoring now after half a bottle of Wild Turkey and listening to LOUD music- a new Joe Walsh record (Analogue Man) - a Thanksgiving tradition!  _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_blink.png_qt_ alt_eq__qt__dd_blink_dd__qt__gt_(Wild Turkey)_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_So here goes!  Wow_co_ I can_t_t remember - The microphone is very important_co_ and the size of the diaphragm in the microphone is somewhat proportional to audio recording quality.  However_co_ for recording human voices_co_ a cardioid headset microphone is sufficient.  There are two primary types - both analogue - phantom powered and low powered microphones.  I prefer a 48 volt phantom powered microphone_co_ which requires a 48 volt power supply which some audio mixers provide (this is best)_co_ but you can also build your own using a couple 9 volt batteries.  I use Audiotechnica and Plantronics headset microphones low power($50 - $100 range) and phantom power($100 and up.)  I have also used Sennheiser headset microphones beginning at around $300 each_co_ but these have shown nominal improvement.  Just make sure if your microphone requires phantom power_co_ as it will produce a low level or no level if plugged into your computer without running through and audio mixer or box that provides phantom power._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_As for audio boards_co_ since all microphones are analogue_co_ the conversion to digital happens on your audio board.  Unless you are using software such as Pro Tools which has strict hardware requirements_co_ my opinion is that most outboard audio boards are a waste and actually provide less versatility.  As all audio boards are either 32 bit or 64 bit_co_ anything that converts the signal to 32 bit or above is fine.  I don_t_t really like USB components for audio_co_ since manipulating the analogue audio signal is much more versatile and far less expensive before your audio board converts the signal to a digital waveform.  What I would avoid is a stationary microphone as the level is difficult to control_co_ and never use the microphone on a laptop (unless you have no alternatives) as this is the very worst in frequency response.  _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_A key component is a small 2 or 4 channel audio mixer.  I use the Mackie 402VLZ4 audio mixer ($99) as it provides me with phantom power for the microphones I use.  And I place an out-board parametric EQ in-line prior to the output of the mixer - but an out-board parametric EQ is a bit expensive.  If I were to go with the least expensive_co_ but highest quality audio output for voice analysis_co_ I would use a Behringer 802 4 channel mixer ($65.)  It doesn_t_t provide phantom power_co_ but does have a single band parametric EQ_co_ and both low and high frequency analogue EQ sweeps - which should do the job._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_You_t_ll also require a 1/8th_qt_ the 1/4_qt_ audio adapter for the microphone (many microphones come with this)_co_ and a mono or stereo cable with one end an 1/4_qt_ audio plug and at the other end an 1/8_qt_ inch plug - which is required to plug into your computer.  I should also point out that mono is fine_co_ however as I_t_m sure you realize_co_ only your left channel will produce an audio signal.  I use mono_co_ as stereo is useless for this recording application._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I know I wrote more earlier today_co_ and hope this post isn_t_t lost to the ages as the last one.  Please let me know if there is any more I might do to assist in this area_co_ and HAVE A GREAT THANKSGIVING!  I_t_m already doing so!   _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_cool.png_qt_ alt_eq__qt_B)_qt__gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Cheers_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_David B._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_strong_gt_Too much Holiday!  I didn_t_t see the page addendum_lt_/strong_gt_. _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_blink.png_qt_ alt_eq__qt__dd_blink_dd__qt__gt__lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2014-11-27T03:49:21Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi Jeff_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Regardless of what the studies show_co_ I use an AX_co_ but AH or AU for the vowel in such a word as HOT is fine.  My personal preferences are certainly not outlinedt in any studies_co_ however in looking at the processing in many voice recognition applications_co_ one of these should be supported.  AH is almost always supported.  If not_co_ as long as the source is editable_co_ you can define these identifiers and write them in yourself.  AA will never suffice for a word such as HOT._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Cheers_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_David_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2014-11-29T01:37:17Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi Jeff_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Just to make my last point clear_co_ my own personal experience has demonstrated to me that AA does not seem to be very reliable in practice - which is why I try and avoid using AA when I can use AO (I hastily wrote AU instead of AO in my last post.)  AA is generally defined in VR software too broadly_co_ and often unconditionally picks up too broad of a range of phonemes.  I like to be as selective as possible_co_ and there_t_s considerable redundancy in the Arpabet as it_t_s defined_co_ and I find even more redundancy in practice_sm_ generally due to differences is dialect_co_ culture_co_ and pronunciation_co_ and also using untrained VR software._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Cheers_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_David_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2014-12-02T16:02:01Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_David_co_ Thanks._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I have a new version coming out soon_co_ I hope.  I_t_ll respond to you when it is ready._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_In the meantime_co_ I was wondering if I have to switch to an .mp3 format to get the upcoming Web Audio support for BJS to play my files?  (Am trying to get both mono &amp_sm_ stereo output for .wav &amp_sm_ it is difficult enough).  I see posts that &lt_sm_audio&gt_sm_ tags for EI do not work with .wav files.  Is this also going to be the case here?  I do not have Windows_co_ so I can not answer this myself._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Thanks_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Jeff_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"}]