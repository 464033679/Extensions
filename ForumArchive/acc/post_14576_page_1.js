[{"Owner":"JCPalmer","Date":"2015-05-17T22:00:52Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_This is an idea I have started to think about.  It is not quite fully baked_co_ so let me say what I think would be a good strategic direction &amp_sm_ why_co_ first._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_At Xmas_co_ I bought myself a new 4x_co_ 3D_co_ Sony TV.  Have not done a lot with the 3D yet_co_ but did run some samples from a channel on Roku_co_ which I have from my prior setup.  Looked amazing.  When I viewed the samples with the 3D off on the TV_co_ they were just 2 pictures side by side.  Am waiting for Sony_t_s 2015 4x upscaling 3D Blue-ray player with _qt_Playstation now_qt_ built in_co_ but I digress._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_There is also a screen mirroring feature on the Sony &amp_sm_ probably other brands.  It uses something called miracast.  _lt_a href_eq__qt_http_dd_//www.howtogeek.com/200796/what-is-miracast-and-why-should-i-care/_qt_ rel_eq__qt_external nofollow_qt__gt_http_dd_//www.howtogeek.com/200796/what-is-miracast-and-why-should-i-care/_lt_/a_gt_ .  What if you could make either a website_co_ or app_co_ which the user could run on a mobile device with a switch to go 3D.  If they had a 3D TV_co_ then it might be unbelievable.  Great for Device orientation_co_ arc rotate_co_ and possible game pad cameras._lt_/p_gt__lt_p_gt_- - - - - - - - -_lt_/p_gt__lt_p_gt_Now the how_dd__lt_/p_gt__lt_p_gt_I have nothing against Oculus_co_ but having a whole set of duplicate cameras just for it is not great.  What if Oculus flops?  Oculus cameras might even be used to achieve this_co_ except for this wierd distortion required.  Might it not be better to refactor into putting an undistorted 3D capability into FreeCamera &amp_sm_ arcRotateCameras_co_ turned on with set3D(true)?  You could also have a setOculus(true)_co_ which also distorts it._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-05-18T15:49:57Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Well_co_ thought I would bake it a little more.  First a correction_co_ the final images ARE distorted.  Went back to this 1 Roku channel that said they doing 3D_co_ WeathTV.  Re-watched these samples_co_ this time with the 3D off_co_ that sort of looked like the old tv show _qt_life of the rich and famous_qt_.  It is obvious that their real camera made 2_co_ 9 x 8 pictures out of 2_co_ 9 x 16_t_s.  I imagine James Cameron_t_s_co_ over-under_co_ Fusion Camera System _lt_a href_eq__qt_http_dd_//en.wikipedia.org/wiki/Fusion_Camera_System_qt_ rel_eq__qt_external nofollow_qt__gt_http_dd_//en.wikipedia.org/wiki/Fusion_Camera_System_lt_/a_gt_ does the same thing in the opposite dimension.  Do not think we would to fix the aspect ratio_co_ just divide what ever the window was._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I took a picture of the screen_co_ which unfortunately did not get it all.  Noticed 2 things_co_ first the pictures are almost identitical spacially.  Yes_co_ things are far in the background_co_ but the 3D text in the foreground starts at nearly the exact same distance from the left._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_2nd_co_ it looks like the 3D effect is produced by the colors being _qt_cooked_qt_ on the right.  It was harder to see on the indoor shots_co_ but it always was there..  Guess that is the other distortion._lt_a href_eq__qt_https_dd_//www.dropbox.com/s/yu7d8vynmap5f8b/2015-05-18%2008.40.33.jpg?dl_eq_0_qt_ rel_eq__qt_external nofollow_qt__gt_https_dd_//www.dropbox.com/s/yu7d8vynmap5f8b/2015-05-18%2008.40.33.jpg?dl_eq_0_lt_/a_gt__lt_/p_gt__lt_p_gt_- - - - - - - - - - -_lt_/p_gt__lt_p_gt_I also looked at Oculus source code.  Probably just leave that alone.  Maybe a redo of anaglyph might be better.  If AnaglyphArcRotateCamera &amp_sm_ AnaglyphFreeCamera could be put inside of ArcRotateCamera &amp_sm_ FreeCamera and generalized more_co_ then a set3D() method could take constants_dd_ (None_co_ Anaglyphic_co_ side-by-side_co_ over-under)._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Maybe the same trick used to turn SIMD on/off could used_co_ which looks like it has no call overhead could be employeed here too._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-05-18T22:45:19Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi Jeff_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_You don_t_t stop thinking_co_ do you?  I drive myself crazy pondering such things - but I supose someone has to.  I went down the road of 3D with Jim on Avatar_co_ and it_t_s now a fairly complex path to something that doesn_t_t completely ruin the experience.  Using _lt_span style_eq__qt_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt_anaglyph imagery is perhaps ok to implement_co_ but it_t_s much easier to grasp what the basic problems with 3 dimensional imagery is in displaying on a 2D screen.  It is a very un-natural process_co_ and can easily confuse the brain trying to interoplate.  Oculus and other similar devices such as the hololens have a short shelve life once newer technologies come to market_sm_ and the hololens isn_t_t even released yet.  This is why Zuckerberg now realizes he made a bonehead decision_co_ as Oculus will never roucoup 2 billion dollars.  That_t_s just insane. _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_blink.png_qt_ alt_eq__qt__dd_blink_dd__qt__gt__lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt_ _lt_/span_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_span style_eq__qt_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt_The retina of the eye can be understood as a plug directly into the brain_co_ since it is one of the two the primary methods we use to collect information.  The new technologies forthcoming in about 2 years don_t_t simulate natural light_co_ but they produce it across fields.  However in the mean time_co_ such tools as using _lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt_anaglyphs might carry us through_co_ but I would be curios if you might take a look at the new technologies in preparing for the very near future. _lt_/span_gt__lt_span style_eq__qt_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt_ We need more good minds pondering this_co_ as the 2D path is already saturated with too many crappy solutions._lt_/span_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt__lt_span style_eq__qt_color_dd_rgb(40_co_40_co_40)_sm_font-family_dd_helvetica_co_ arial_co_ sans-serif_sm__qt__gt_DB_lt_/span_gt__lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-05-19T15:22:10Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_DB_co__lt_/p_gt__lt_p_gt_Is there anybody you don_t_t know?  For a second there_co_ I thought you said_co_ by ommision_co_ that Oculus Rift was on the market.  The site say Q1 2016.  Did some break even math_co_ 2 billon / $350 per unit _eq_ 5.7 million units (@ 100% margin).  Just because it will not payoff does not mean they would shut it down &amp_sm_ eat the whole thing_co_ though.  I think you have too much invested in Magic Leap to be objective.  Enough said on the non-anaglyph front._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Anaglyph has been around since 1852.  Given that 3D movies have made a comeback_co_ after iMax kept them alive for decades_co_ people want to see them in their homes in 3D too.  I think BJS would do well to upgrade the anaglyph and put it inside the Camera base class.  Make it overridable for say Arc rotate.  I can think of other implemenations where this could payoff as well.  Since TVs_co_ can switch to processing any input as Side-by-side or above-below 3D_co_ then 3D console games are a potential.  I am pretty sure BabylonHx ports to them. _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Being inside the Camera base class means you only have to put up UI before the games actually gets going_co_ to allow the customer to switch it on.  All cameras would also have it_co_ not just Free &amp_sm_ arc rotatate_co_ but gamepad_co_ virtual joystick_co_ follow_co_ etc._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_If you have any insite on the Fusoin camera system that would apply to virtual cameras_co_ or other tips on implementing this_co_ I am all ears._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Jeff_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-05-19T20:19:39Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Hi Jeff_co__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I have been very lucky to work with alot of great people.  But that_t_s what happens when you get into technology early_co_ ad work 15-20 hours a day for 20 years_sm_ sometimes three productions at once._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I can tell you about the Fusion camera system_co_ as I also knw Vince Pace at Pace Productions in Burbank where the Fusion system was developed.  Jim is a partner in the company_co_ and has allowed Vice to pursue 3D beyond what anyone else is doing.  If you ever visit them_co_ they have hundreds of 3D cameras_co_ and usually more than half of them are rented out at any given time.  Sometimes they are supporting a hundred productions at once.  But Vince is always Jim_t_s Cinematographer.  To use the Fusion system_co_ you must have two cameras at a specific distance and completely synced together in a single moving unit.  Only if you replicate this and convince Vince and Jim to relaese the interpolation software to the public_co_ I don_t_t see how you might use Fusion in Web\\GL.  However_co_ it wouldn_t_t be that difficult to replicate what Vince_t_s team has built in WebGL.  You simply need to understand the optics_co_ and then render two cameras at a fixed distance in your scene._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_As for Oculus_co_ I don_t_t know why they haven_t_t released the Rft yet_co_ as they have thousands of Rifts available for development which are already mass produced.  At GDC there was hundreds on the floor.  So I_t_ve been testing one for almost a year and I can_t_t stand using it.  I thought they were releasing it this year_co_ and there are several competitors who are releasing this year for less money.  So I think they are incredibly poor at business_co_ unfortunately.  Even when they release_co_ it might be a bit passe_t_.  But I gave the students at UCSB an introduction to what Magic Leap is building_co_ and they all were bumfounded.  So I recommend taking a look at Magic Leap for the not too distant future_co_ but also realize that the computation in rendering light fields is incredibly CPU heavy.  _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_But I belive that within 4-5 years_co_ this won_t_t need to be post_co_ but the CPU speed will render in real time which is why I_t_m working in WebGL.  I just bought a new tablet from Sony - and Experion V2_co_ and I have to sa that this is amazing.  It_t_s only 6mm thick ( about the thickness of a nickel )_co_ can be used for hours under water without fail_co_ has a 2.3 Ghz quad core processor with 3MB of RAM on the processor_co_ has a very fast ARM GPU_co_ and runs as fast as my new ASUS laptop.  SO were getting really close_co_ and Magic Leap has hired the co-creator of the XBOX which I met with 2 weeks ago.  Hardware appears to be the least of our problems at this time._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I hope this info helps._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_DB_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Deltakosh","Date":"2015-05-20T17:36:48Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Actually if you look at the code of Oculuscamera there is no that much amout of code. The interesting camera is the WebVR who is targeted to be more standard (The only difference is how we handle the inputs)_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I like your idea of having the distorded rendering baked into Babylon.Camera itself letting children only control the input. But we have to think about how we can define specific values like fov_co_ eye distance and so on._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_So for me this is a great idea which just need to be shaped_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-05-20T18:16:41Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_What Vince does with distance between the cameras is the very same proportion to the human pair of eyes.  This is called the IPD or interpupillary distance.  When you are fitted for eyeglasses_co_ this is measured and there is an average range that exists for humans.  This range is 54mm to 68mm_co_ and we typically default to 60mm in general calculations.  A child_t_s IPD can be as narrow as 48mm_co_ and I would leave the IPD for VR cameras as a variable.  This is less dependant on the person than it is the scene parameters.  So anyone who would want to use this _qt_new_qt_ camera would adjust the IPD and focal lenght custom to their scene based on focal length and distance to objects in the scene.  _lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_In working on stereoscopic film conversion in the past_co_ we adjust the IPD as a fixed (constant) setting for a scene_co_ and it works for all people as the pupils can interpolate this on an (x_co_y) plane (camera plan.)_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_It should be very easy to simply add this variable to the VR camera_co_ and allow it to be set by the developer based upon their own preferences and the device they are developing for._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-05-20T18:59:16Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_I am not ready for the stuff talked being about yet.  I am 5 hrs into implementing.  I am doing all the refactoring_co_ and straightenout out of stuff_co_ like NO ONE except for Camera implements _update().  _update() is now defined on Camera as_dd__lt_/p_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_public _update()_dd_ void {    this._checkInputs()_sm_    if (this._subCameraMode !_eq__eq_ Camera._SUB_CAMS_NONE){        this._updateSubCameras()_sm_    }}public _checkInputs()_dd_ void {            }_lt_/pre_gt__lt_p_gt_All the _update() methods in subclasses have been renamed _checkInputs()._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_In order to get things to compile_co_ I have to avoid using private members in Camera that are duplicates of the old cameras_co_ e.g. _leftCamera.  Since left and right are not always valid_co_ I switched to A &amp_sm_ B_co_ and they are just an index into subCameras.  All new statics are_dd__lt_/p_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_private static _SUB_CAMS_NONE _eq_ 0_sm_private static _SUB_CAMS_ANAGLYPH _eq_ 1_sm_private static _SUB_CAMS_HORIZ_STEREOGRAM _eq_ 2_sm_private static _SUB_CAMS_VERT_STEREOGRAM _eq_ 3_sm_private static _SUB_CAMS_OCULUS _eq_ 4_sm_private static _SUB_CAM_A _eq_ 0_sm_private static _SUB_CAM_B _eq_ 1_sm__lt_/pre_gt__lt_p_gt_The 2 stereograms are implement right now without a post process_co_ just viewports.  I hope to get sample scene up_co_ then figure out stuff like fov &amp_sm_ should there be a color brightener on the B sub camera_co_ for stereograms._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-05-20T21:24:14Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p style_eq__qt_color_dd_rgb(51_co_51_co_51)_sm_font-family_dd__t_Myriad Pro_t__co_ Arial_co_ Helvetica_co_ sans-serif_sm_font-size_dd_16px_sm__qt__gt_I_t_m certain you_t_re not ready to implement more at this time_co_ but for future reference here is some additional info I came across for what they are calculating within the Fusion algorithm._lt_/p_gt__lt_p style_eq__qt_color_dd_rgb(51_co_51_co_51)_sm_font-family_dd__t_Myriad Pro_t__co_ Arial_co_ Helvetica_co_ sans-serif_sm_font-size_dd_16px_sm__qt__gt_The Depth Bracket of your scene refers to the actual distance between your closest object in the frame and the furthest object.  The Parallax Budget refers to your calculated maximum positive parallax and desired maximum negative parallax represented in percentage of screen width.  For example if I determine through a simple calculation that my positive parallax should never exceed 0.7% of screen width and I have determined that my negative parallax should not exceed 2% of screen width_co_ then my total Parallax Budget is 2.7%.   The Depth Bracket must be able to be squeezed into the Parallax Budget.  There are many algebraic formulas to determine the proper interaxial distance to achieve this._lt_/p_gt__lt_p style_eq__qt_color_dd_rgb(51_co_51_co_51)_sm_font-family_dd__t_Myriad Pro_t__co_ Arial_co_ Helvetica_co_ sans-serif_sm_font-size_dd_16px_sm__qt__gt_The native parallax for a given screen size simply refers to what percentage of screen width will equal the human interocular.  If you are using 2.5 inches as the baseline interocular and you know your presentation screen will be 30 feet wide (360 inches) then just divide 2.5 by 360.  _lt_strong_gt_2.5_lt_/strong_gt_ _lt_strong_gt_÷ 360 _eq_ 0.007 or 0.7%_lt_/strong_gt_  Therefore the Native Parallax of a 30 foot screen is 0.7%_co_ so we should make sure to keep our maximum positive parallax under 0.7% of screen width if we plan to show our footage on a 30 foot wide screen.  If we shoot for a 65” 3DTV_co_ then we can get away with over 3% positive parallax._lt_/p_gt__lt_p style_eq__qt_color_dd_rgb(51_co_51_co_51)_sm_font-family_dd__t_Myriad Pro_t__co_ Arial_co_ Helvetica_co_ sans-serif_sm_font-size_dd_16px_sm__qt__gt_The 1/30 rule refers to a commonly accepted rule that has been used for decades by hobbyist stereographers around the world.  It basically states that the interaxial separation should only be 1/30_lt_span style_eq__qt_margin_dd_0px_sm_font-size_dd_10px_sm__qt__gt_th_lt_/span_gt_ of the distance from your camera to the closest subject.  In the case of ortho-stereoscopic shooting that would mean your cameras should only be 2.5” apart and your closest subject should never be any closer than 75 inches (about 6 feet) away._lt_/p_gt__lt_p style_eq__qt_color_dd_rgb(51_co_51_co_51)_sm_font-family_dd__t_Myriad Pro_t__co_ Arial_co_ Helvetica_co_ sans-serif_sm_font-size_dd_16px_sm_text-align_dd_center_sm__qt__gt__lt_strong_gt_Interaxial x 30 _eq_ minimum object distance_lt_br_gt_or_lt_br_gt_Minimum object distance ÷ 30 _eq_ Interaxial_lt_/strong_gt__lt_/p_gt__lt_p style_eq__qt_color_dd_rgb(51_co_51_co_51)_sm_font-family_dd__t_Myriad Pro_t__co_ Arial_co_ Helvetica_co_ sans-serif_sm_font-size_dd_16px_sm__qt__gt_If you are using a couple standard 6″ wide camcorders in a side by side rig as close as they will fit together then the calculation would look like_dd_ 6” x 30 _eq_ 180 inches or 15 feet.  _lt_/p_gt__lt_p style_eq__qt_color_dd_rgb(51_co_51_co_51)_sm_font-family_dd__t_Myriad Pro_t__co_ Arial_co_ Helvetica_co_ sans-serif_sm_font-size_dd_16px_sm__qt__gt_The 1/30 rule certainly does not apply to all scenarios.  In fact_co_ in feature film production destined for the big screen we will typically use a ratio of 1/60_co_ 1/100 or higher.  The 1/30 rule works well if your final display screen size is less than 65 inches wide_co_ your cameras were parallel to each other_co_ and your shots were all taken outside with the background at infinity.  When you are ready to take the next step to becoming a stereographer you will need to learn about parallax range and the various equations available to calculate maximum positive parallax (the parallax of the furthest object_co_) which will translate into a real-world distance when you eventually display your footage._lt_/p_gt__lt_p style_eq__qt_color_dd_rgb(51_co_51_co_51)_sm_font-family_dd__t_Myriad Pro_t__co_ Arial_co_ Helvetica_co_ sans-serif_sm_font-size_dd_16px_sm__qt__gt_DB_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-05-21T17:52:02Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Well_co_ I am close to a working sample scene.  The buttons for _t_Stereogram Horizontal_t_ &amp_sm_ _t_Vertical_t_ each do the oppose_co_ but they do it.  _t_None_t_ oddly makes gryff_t_s zombie disappear.  Then there is the Dialog System invading the scene.  This is because in the layer mask work done for specialty cameras_co_ Camera_t_s default was left at 0xFFFFFFFF.  The Dialog System merely changes scene cameras to match the mesh default_co_ 0x0FFFFFFF.  I did not even know a about subcameras till 2 days ago.  I am in that file_co_ so I am just going to change the Camera default to fix._lt_/p_gt__lt_p_gt__lt_a class_eq__qt_ipsAttachLink ipsAttachLink_image_qt_ href_eq__qt_http_dd_//www.html5gamedevs.com/uploads/monthly_05_2015/post-8492-0-55111300-1432229736.png_qt_ rel_eq__qt_external nofollow_qt__gt__lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/monthly_05_2015/post-8492-0-55111300-1432229736.png_qt_ data-fileid_eq__qt_4332_qt_ class_eq__qt_ipsImage ipsImage_thumbnailed_qt_ alt_eq__qt_post-8492-0-55111300-1432229736.png_qt__gt__lt_/a_gt__lt_/p_gt__lt_p_gt_I will post link to scene once above fixed.  Leaving some more thoughts on how it went till then._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_DB_co_ just an explanation of how I got to the Fusion Camera System.  In 2009_co_ I saw a James Cameron interview on a cable TV show _lt_a href_eq__qt_http_dd_//www.g4tv.com/videos/48219/Avatars-Cameron-Pace-3D-Camera-Rig-Review/_qt_ rel_eq__qt_external nofollow_qt__gt_http_dd_//www.g4tv.com/videos/48219/Avatars-Cameron-Pace-3D-Camera-Rig-Review/_lt_/a_gt_.  He showed a camera which had up-down cameras_co_ not right-left.  Fast forward to when I was making my first post in this thread.  The Sony has an up-down 3D setting.  I remembered the show.  I looked up _t_Avatar_t_ &amp_sm_ saw Fusion was used.  Do not know yet how much of this translates to the virtual world_co_ but it is good to have that info_co_ thanks._lt_/p_gt__lt_p_gt_ _lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-05-22T17:24:34Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_To start here is the link_dd_ _lt_a href_eq__qt_https_dd_//googledrive.com/host/0B6-s6ZjHyEwUfkRGTUVVTUxaT01UZUlieGdCODF1blFZZHJFMnRJUHBpcG1KcnBjUVluYXc_qt_ rel_eq__qt_external nofollow_qt__gt_https_dd_//googledrive.com/host/0B6-s6ZjHyEwUfkRGTUVVTUxaT01UZUlieGdCODF1blFZZHJFMnRJUHBpcG1KcnBjUVluYXc_lt_/a_gt__lt_/p_gt__lt_p_gt_Took longer_co_ since my last use of viewport was using orthographic cameras.  You can stretch / compress easily with camera just using viewport.  After I fixed all those minor problems_co_ I realized that the stereograms were just drawn with a different window size_co_ not compressed._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I figured out after a while I would need a post process.  The stereogram vertical seemed easy.  The zombie was now smaller in height as required_co_ but unfortunately smaller in width.  Here i just mapped the x into .25 to .75.  But cannot figure out what to about stereogram horizontal.   Any thoughts? Here is the fragment shader_dd__lt_/p_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_#ifdef GL_ESprecision highp float_sm_#endif// Samplersvarying vec2 vUV_sm_uniform sampler2D textureSampler_sm_uniform float isStereogramHoriz_sm_void main(void){    vec2 coord_sm_\tif (isStereogramHoriz _eq__eq_ 1.0)            coord _eq_ vec2(vUV.x_co_ vUV.y)_sm_\telse\t    coord _eq_ vec2(0.25 + vUV.x / 2.0_co_ vUV.y)_sm_\tgl_FragColor _eq_ vec4(texture2D(textureSampler_co_ coord).rgb_co_ 1.0)_sm_}_lt_/pre_gt__lt_p_gt_I put in a switch to skip postprocessing_co_ just to see how the Stereogram vertical looks without the compression._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Also most of the code is added at the bottom of camera._lt_/p_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_// ---- 3D cameras section ----// skipPostProcess is a temp argpublic setSubCameraMode(mode _dd_ number_co_ halfSapce _dd_ number_co_ skipPostProcess? _dd_ boolean) _dd_ void{    // not likely in production that any prior sub cams_co_ but in dev maybe     while (this.subCameras.length &gt_sm_ 0){        this.subCameras.pop().dispose()_sm_     }                             this._subCameraMode _eq_ mode_sm_        this._subCamHalfSapce _eq_ Tools.ToRadians(halfSapce)_sm_                 var camA _eq_ this.GetSubCamera(this.name + _qt__A_qt__co_ true )_sm_     var camB _eq_ this.GetSubCamera(this.name + _qt__B_qt__co_ false)_sm_     var postProcessA _dd_ PostProcess_sm_     var postProcessB _dd_ PostProcess_sm_                 switch (this._subCameraMode){        case Camera._SUB_CAMS_ANAGLYPH_dd_            postProcessA _eq_ new PassPostProcess(this.name + _qt__leftTexture_qt__co_ 1.0_co_ camA)_sm_            camA.isIntermediate _eq_ true_sm_                                postProcessB _eq_ new AnaglyphPostProcess(this.name + _qt__anaglyph_qt__co_ 1.0_co_ camB)_sm_            postProcessB.onApply _eq_ effect _eq_&gt_sm_ {                effect.setTextureFromPostProcess(_qt_leftSampler_qt__co_ postProcessA)_sm_            }_sm_            break_sm_                            case Camera._SUB_CAMS_HORIZ_STEREOGRAM_dd_            camA.viewport _eq_ new Viewport(  0_co_   0_co_ 0.5_co_ 1.0)_sm_            if (!skipPostProcess) postProcessA _eq_ new StereogramCompressionPostProcess(_qt_horiz comp left_qt__co_ camA_co_ true)_sm_                                                    camB.viewport _eq_ new Viewport(0.5_co_   0_co_ 0.5_co_ 1.0)_sm_            if (!skipPostProcess) postProcessB _eq_ new StereogramCompressionPostProcess(_qt_horiz comp rite_qt__co_ camB_co_ true)_sm_                                break_sm_                            case Camera._SUB_CAMS_VERT_STEREOGRAM_dd_            camA.viewport _eq_ new Viewport(  0_co_   0_co_ 1.0_co_ 0.5)_sm_            if (!skipPostProcess) postProcessA _eq_ new StereogramCompressionPostProcess(_qt_vert comp top_qt_ _co_ camA_co_ false)_sm_                                  camB.viewport _eq_ new Viewport(  0_co_ 0.5_co_ 1.0_co_ 0.5)_sm_            if (!skipPostProcess) postProcessB _eq_ new StereogramCompressionPostProcess(_qt_vert comp bot_qt_ _co_ camB_co_ false)_sm_              break_sm_                            case Camera._SUB_CAMS_OCULUS_dd_            camA.viewport _eq_ new Viewport(  0_co_   0_co_ 0.5_co_ 1.0)_sm_            camA._OculusWorkMatrix _eq_ new Matrix()_sm_                               camA._OculusHMatrix _eq_ OculusLeftHMatrix_sm_            camA._OculusPreViewMatrix _eq_ OculusLeftPreViewMatrix_sm_                                camA.getProjectionMatrix _eq_ camA.getOculusProjectionMatrix_sm_            postProcessA _eq_ new OculusDistortionCorrectionPostProcess(_qt_Oculus Distortion Left_qt__co_ camA_co_ false_co_ OculusRiftDevKit2013_Metric)_sm_                                camB.viewport _eq_ new Viewport(0.5_co_   0_co_ 0.5_co_ 1.0)_sm_            camB._OculusWorkMatrix _eq_ new Matrix()_sm_            camB._OculusHMatrix _eq_ OculusRightHMatrix_sm_            camB._OculusPreViewMatrix _eq_ OculusRightPreViewMatrix_sm_                                camB.getProjectionMatrix _eq_ camB.getOculusProjectionMatrix_sm_            postProcessB _eq_ new OculusDistortionCorrectionPostProcess(_qt_Oculus Distortion Right_qt__co_ camB_co_ true _co_ OculusRiftDevKit2013_Metric)_sm_    }    if (this._subCameraMode !_eq__eq_ Camera._SUB_CAMS_NONE){        this.subCameras.push(camA)_sm_        this.subCameras.push(camB)_sm_    }    this._update()_sm_}        private getOculusProjectionMatrix()_dd_ Matrix {    Matrix.PerspectiveFovLHToRef(OculusAspectRatioFov_co_ OculusAspectRatio_co_ this.minZ_co_ this.maxZ_co_ this._OculusWorkMatrix)_sm_    this._OculusWorkMatrix.multiplyToRef(this._OculusHMatrix_co_ this._projectionMatrix)_sm_    return this._projectionMatrix_sm_}        /** * needs to be overridden in ArcRotateCamera &amp_sm_ TargetCamera_co_ so sub has required properties to be copied */public GetSubCamera(name _dd_ string_co_ isA _dd_ boolean) _dd_ Camera{     return null_sm_  }        /** * needs to be overridden in ArcRotateCamera_co_ adding copy of alpha_co_ beta &amp_sm_ radius * needs to be overridden in TargetCamera_co_ adding copy of position_co_ and rotation for Oculus_co_ or target for rest */public _updateSubCameras(){    var camA _eq_ this.subCameras[Camera.SUB_CAM_A]_sm_    var camB _eq_ this.subCameras[Camera.SUB_CAM_B]_sm_    camA.minZ _eq_ camB.minZ _eq_ this.minZ_sm_    camA.maxZ _eq_ camB.maxZ _eq_ this.maxZ_sm_    camA.fov  _eq_ camB.fov  _eq_ this.fov_sm_ // original Oculus did not do this                // only update viewport_co_ when ANAGLYPH    if (this._subCameraMode _eq__eq__eq_ Camera.SUB_CAMS_ANAGLYPH){        camA.viewport _eq_ camB.viewport _eq_ this.viewport_sm_                    }}        _lt_/pre_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-05-22T23:17:26Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_You don_t_t need to compress_co_ as both images should be interlaced and end up very close to the same resolution upon display.  I don_t_t see where you are rendering every other line and then interpolating to interlaced images.  This why they use vertical cameras_co_ so that the camera is easier to manage in the real world.  I hope this makes sense._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-05-23T12:53:23Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Oh yea_co_ that helped.  Scrap the viewports like the Oculus process was using &amp_sm_ switch to a process more similar to ANAGLYPH.  Make camA not actually display with _lt_span_gt_isIntermediate _lt_/span_gt__lt_span_gt__eq__lt_/span_gt__lt_span_gt_ _lt_/span_gt__lt_span_gt_true_lt_/span_gt__lt_span_gt_. Not sure if I can get away with just one shader._lt_/span_gt_ I see camA_t_s frame buffer is passed to camB_t_s postprocess via_co_ _lt_span_gt_effect_lt_/span_gt__lt_span_gt_._lt_/span_gt__lt_span_gt_setTextureFromPostProcess.  Will check if there is something like effect.setTextureFromCamera()._lt_/span_gt__lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Will not get to this till later_co_ but I smell blood.  This is going to work!  I will also change sample scenes to one that is more _qt_gonso_qt_.  Want some actual movement of meshes forground-background wise.  Have a flying tablecloth test scene too._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-05-24T22:45:15Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Ok_co_ I updated the link.  Please forgive this long email_co_ but tomorrow is a holiday &amp_sm_ I am just doing a memory dump._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_First_co_ here is the shader &amp_sm_ subset of _lt_span_gt_setSubCameraMode() _lt_/span_gt_ to replace the ones above_dd__lt_/p_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_#ifdef GL_ESprecision highp float_sm_#endifconst vec3 TWO _eq_ vec3(2.0_co_ 2.0_co_ 2.0)_sm_varying vec2 vUV_sm_uniform sampler2D camASampler_sm_uniform sampler2D textureSampler_sm_uniform bool isStereogramHoriz_sm_uniform vec2 stepSize_sm_void main(void){    bool useCamB_sm_    vec2 texCoord1_sm_    vec2 texCoord2_sm_        vec3 frag1_sm_    vec3 frag2_sm_        // outer if branch should have no impact at all_co_ since fragments will ALWAYS take the same branch\tif (isStereogramHoriz){\t    useCamB _eq_ vUV.x &gt_sm_ 0.5_sm_\t    texCoord1 _eq_ vec2(useCamB ? (vUV.x - 0.5) * 2.0 _dd_ vUV.x * 2.0_co_ vUV.y)_sm_\t    texCoord2 _eq_ vec2(texCoord1.x + stepSize.x_co_ vUV.y)_sm_\t}else{\t    useCamB _eq_ vUV.y &gt_sm_ 0.5_sm_\t    texCoord1 _eq_ vec2(vUV.x_co_ useCamB ? (vUV.y - 0.5) * 2.0 _dd_ vUV.y * 2.0)_sm_\t    texCoord2 _eq_ vec2(vUV.x_co_ texCoord1.y + stepSize.y)_sm_\t}        // cannot assign a sampler to a variable_co_ so must duplicate texture accesses    if (useCamB){        frag1 _eq_ texture2D(textureSampler_co_ texCoord1).rgb_sm_        frag2 _eq_ texture2D(textureSampler_co_ texCoord2).rgb_sm_    }else{        frag1 _eq_ texture2D(camASampler   _co_ texCoord1).rgb_sm_        frag2 _eq_ texture2D(camASampler   _co_ texCoord2).rgb_sm_    }        gl_FragColor _eq_ vec4((frag1 + frag2) / TWO_co_ 1.0)_sm_}_lt_/pre_gt__lt_p_gt_and_lt_/p_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_switch (this._subCameraMode){    case Camera._SUB_CAMS_ANAGLYPH_dd_        ...                        case Camera._SUB_CAMS_HORIZ_STEREOGRAM_dd_    case Camera._SUB_CAMS_VERT_STEREOGRAM_dd_        var isStereogramHoriz _eq_ this._subCameraMode _eq__eq__eq_ Camera._SUB_CAMS_HORIZ_STEREOGRAM_sm_        postProcessA _eq_ new PassPostProcess(_qt_passthru_qt__co_ 1.0_co_ camA)_sm_          camA.isIntermediate _eq_ true_sm_                            postProcessB _eq_ new StereogramInterlacePostProcess(_qt_st_interlace_qt_ _co_ camB_co_ postProcessA_co_ isStereogramHoriz)_sm_          break_sm_                        case Camera._SUB_CAMS_OCULUS_dd_        ...}_lt_/pre_gt__lt_p_gt_I did not get to doing a better scene_co_ but will work on it._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Think that I should be PRing this as is.  We are not going to get this in one shot no matter what.  I do not even have an Oculus Rift.  There are # of things still to do_dd__lt_/p_gt__lt_ol_gt__lt_li_gt_Test Oculus still works on an actual device._lt_/li_gt_\t_lt_li_gt_Either delete un-needed Oculus classes_co_ or hollow them out to a couple of lines.  Would actually recommend deleting them.  Oculus is not a real product yet.  Devs must change a box saying so to buy one.  Get rid of the garbage now.  Also_co_ that webVR camera probably should not be hardcoded to only work with Oculus.  Another reason to delete them._lt_/li_gt_\t_lt_li_gt_Delete or hollow out Anaglyph camera classes.  Does anyone really use them?_lt_/li_gt_\t_lt_li_gt_Test the stereograms.  I have hardware_co_ fuckin iPad_co_ issues.  More in a later section.  Beyond the initial test of seeing a single image with 3D turned on_co_ need intelligent ways to set separation.  I just have this hard coded in the test scene &amp_sm_ have no idea if I got lucky or not._lt_/li_gt_\t_lt_li_gt_As I worked with the Cameras_co_ I think it would really be good if Target Camera could be collapsed into Camera.  There are only 2 Cameras that directly subclass Camera_co_ Target &amp_sm_ ArcRotate.  Those cameras have _getViewMatrix()_co_ so the one for Oculus needs to be swapped out in the subclasses.  ArcRotate has to have its own _getViewMatrix()_co_ but might be eliminatable._lt_/li_gt__lt_/ol_gt__lt_p_gt_As I said before_co_ I do not have all the hardware to test this.  I have a 3D TV_co_ but iPad does not support Mira-cast.  iPad_t_s only way to get to a TV is through an Apple TV box.  I am not buying that.  That_t_s all I need_co_ another way to get Netflix.  Apple also _qt_leaked_qt_ this week_co_ that they have abandoned making an actual TV_co_ but are working on a new version of Apple TV.  I need another way._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I was always going to get an Android tablet for testing.  Had been delaying_co_ since my aging Samsung Galaxy s3 was good enough till now.  It kind of has Mira-cast_co_ but it is just garbage artifacts upon connection.  DB_co_ I like you have a Sony_co_ since I probably would not feel good about importing a non-US product.  Mira-cast is still fairly new.  If anything is going to work with a Sony TV though_co_ it is going to be a Sony tablet.  Think the 8_qt_ form factor gives me the best coverage.  Where did you get yours?_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-05-24T23:37:54Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_This is a good start.  Rendering from 2 seperate views can be used for practically any devidce or application.  The only elements that need to be user variables depending on scene objects is the IPD - distance between cameras and the FOV._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Deltakosh","Date":"2015-05-25T21:46:09Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_So_dd_ I agree on OculusCamera is no more needed._lt_/p_gt__lt_p_gt_I disagree on Anaglyph_co_ there are users out there_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Other point here_dd_ please do not use _qt_if_qt_ inside shaders but #define. This is a great optimization_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Deltakosh","Date":"2015-05-25T21:59:16Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Actually for Oculus I was thinking about removing it from 2.1_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-05-25T22:38:04Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_I_t_ll respond more tomorrow_co_ but updated the scene with one that is a much better test.  On Oculus_co_ yea try in here.  It does not work.  I double checked by setting to the OculusCamera_co_ from VR dir.  Same result.  Enjoy._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-05-26T01:02:14Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_You don_t_t need to support analglyph as rendering two views can be processed for any use including analglyph.  It will be up to the user to insert the process for the stereo mechanism._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-05-26T18:35:08Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Ok_co_ more time.  Yes_co_ I am now using #define for _lt_span_gt_isStereogramHoriz.  DB_co_ think he wants to cause no changes for anaglyph users.  There is the hollow out way though_dd__lt_/span_gt__lt_/p_gt__lt_pre class_eq__qt_ipsCode prettyprint_qt__gt_export class AnaglyphFreeCamera extends FreeCamera {    constructor(name_dd_ string_co_ position_dd_ Vector3_co_ eyeSpace_dd_ number_co_ scene_dd_ Scene) {        super(name_co_ position_co_ scene)_sm_        this.setSubCameraMode(Camera.SUB_CAMS_ANAGLYPH_co_ eyeSpace)_sm_    }}_lt_/pre_gt__lt_p_gt_If Oculus is being dropped from BJS altogether_co_ that would eliminate 1_co_2_co_ &amp_sm_ 5 from the todo list above.  Please let me know_co_ so I can pull Oculus out of my new way too.  Unless you really meant _qt_hollow out_qt_ like above.  Also that WebVRCamera might be still be good_co_ adding your own 3D mode or not.  If deleting_co_ I will just pack away my own copy._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Just to bring this up again.  If there was a way to avoid the PassPostProcess_co_ that would seem to cut out a draw of the whole screen._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Finally_co_ controlling the thing.  My take on it is we want to make fully operational scenes_co_ assuming the customer does not have any 3D equipment.  If UI is put up at the beginning allowing the customer to customize the scene_co_ the developer can switch it on as required.  Even if the customer does not have the equipment_co_ they would probably view being asked in a positive light_co_ and would now know the scenes can capabilities they are not even using yet._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_To that end_co_ hiding direct access to _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt_fov_lt_/strong_gt__lt_/span_gt_ &amp_sm_ maybe _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt_minZ_lt_/strong_gt__lt_/span_gt_ / _lt_strong_gt__lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt_maxZ_lt_/span_gt__lt_/strong_gt__co_  using getters / setters_co_ would allow us to add code to set _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt__subCamHalfSapce_lt_/strong_gt__lt_/span_gt_ to the best value.  I do not know what that code would be_co_ but know there should be minimuim performance issues.  This stuff is probably changed very infrequently.  Any thoughts on this_co_ or what the _qt_code_qt_ should be?_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Deltakosh","Date":"2015-05-26T18:44:21Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Oculus was removed during the last commit. WebVR and VR DeviceOrientation are still in obviously_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Deltakosh","Date":"2015-05-26T18:45:51Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Oh and I_t_m perfectly fine with using getters and setters. This performance penalty is near zero here_lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Why do you need the PasPostProcess so far? I don_t_t need to use if for Oculus for isntance_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"dbawel","Date":"2015-05-26T19:04:23Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_Depending on whether the images are to be analglyph_co_ interlaced for a single display or on two displays is then up to the user.  This allows the stereo camera to work for all devices and scenarios._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"JCPalmer","Date":"2015-05-26T19:54:39Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_I have not pulled from repo in a week_co_ since I have changed so many files_co_ &amp_sm_ did not want to cause a merge conflict on my repo till I had to_dd__lt_/p_gt__lt_ul_gt__lt_li_gt_babylon._lt_strong_gt_camera_lt_/strong_gt_.ts - implemented most everything_lt_/li_gt__lt_li_gt_babylon._lt_strong_gt_arcRotateCamera_lt_/strong_gt_.ts - renamed _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt__update_lt_/strong_gt__lt_/span_gt_() to _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt__checkInputs_lt_/strong_gt__lt_/span_gt_()_sm_ added new functions _lt_strong_gt__lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt_GetSubCamera_lt_/span_gt__lt_/strong_gt_() &amp_sm_ _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt__updateSubCameras_lt_/strong_gt__lt_/span_gt_()_lt_/li_gt__lt_li_gt_babylon._lt_strong_gt_targetCamera_lt_/strong_gt_.ts - added new functions _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt_GetSubCamera_lt_/strong_gt__lt_/span_gt_()_co_ _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt__updateSubCameras_lt_/strong_gt__lt_/span_gt_()_co_ _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt__getSubCamPosition_lt_/strong_gt__lt_/span_gt_()_co_ &amp_sm_ _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt__getOculusViewMatrix_lt_/strong_gt__lt_/span_gt_()_lt_/li_gt__lt_li_gt_babylon._lt_strong_gt_followCamera_lt_/strong_gt_.ts - renamed _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt__update_lt_/strong_gt__lt_/span_gt_() to _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt__checkInputs_lt_/strong_gt__lt_/span_gt_()_lt_/li_gt__lt_li_gt_babylon._lt_strong_gt_freeCamera_lt_/strong_gt_.ts - deleted _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt__update_lt_/strong_gt__lt_/span_gt_()_lt_/li_gt__lt_li_gt_babylon._lt_strong_gt_Gulfile_lt_/strong_gt_.js - added stereogramInterlacePostProcess.ts_lt_/li_gt__lt_/ul_gt__lt_p_gt_added_dd__lt_/p_gt__lt_ul_gt__lt_li_gt_babylon._lt_strong_gt_stereogramInterlacePostProcess_lt_/strong_gt_.ts_lt_/li_gt__lt_li_gt__lt_strong_gt_stereogramInterlace_lt_/strong_gt_.fragment.fx_lt_/li_gt__lt_/ul_gt__lt_p_gt_So_co_ I did not know you made changes in vr dir.  Looking from GitHub_co_ you can rip out a lot more.  All that innercamera class stuff is no longer needed. Cameras should also not implement _lt_span style_eq__qt_color_dd_#0000ff_sm__qt__gt__lt_strong_gt__update_lt_/strong_gt__lt_/span_gt_() anymore.  Think I should be doing commits_co_ and at least a trial PR.  _lt_span style_eq__qt_color_dd_#ff0000_sm__qt__gt__lt_strong_gt_Please advise_lt_/strong_gt__lt_/span_gt_._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_I made stereogram process much closer to Anaglyph_co_ &amp_sm_ does not use viewports like the Oculus process.  Reason is_co_ I want the postprocess to reduce the 2 subcameras.  Then I have the full sizes so that I can average the rows or columns being interlaced.  Hence the passthru question.  Have not measured it_co_ but Oculus seems like a dog compared to others.  When you switch to it in the tester_co_ both sides don_t_t even switch in the same frame._lt_/p_gt__lt_p_gt_ _lt_/p_gt__lt_p_gt_Also_co_ that stupid _lt_strong_gt_oculusGamepadCamera.js_lt_/strong_gt_ keeps coming back in the camera directory._lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Deltakosh","Date":"2015-05-26T21:22:57Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_let_t_s go for a PR_co_ I_t_ll find a way to merge _lt_img src_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/default_smile.png_qt_ alt_eq__qt__dd_)_qt_ srcset_eq__qt_http_dd_//www.html5gamedevs.com/uploads/emoticons/smile@2x.png 2x_qt_ width_eq__qt_20_qt_ height_eq__qt_20_qt__gt__lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"}]