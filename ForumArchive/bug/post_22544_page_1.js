[{"Owner":"JCPalmer","Date":"2016-05-13T18:36:07Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t\n_lt_p_gt_\n\tUnless I am interpreting what the gradient arg to Matrix.Lerp wrong_co_ it seems to go backwards.  I changed it to this_dd_\n_lt_/p_gt_\n\n_lt_pre_gt_\n_lt_code_gt_public static Lerp(startValue_dd_ Matrix_co_ endValue_dd_ Matrix_co_ gradient_dd_ number)_dd_ Matrix {\n    var result _eq_ Matrix.Zero()_sm_\n\n    for (var index _eq_ 0_sm_ index &lt_sm_ 16_sm_ index++) {\n        result.m[index] _eq_ startValue.m[index] + ((endValue.m[index] -  startValue.m[index]) * gradient)_sm_\n    }\n\n    return result_sm_\n}_lt_/code_gt__lt_/pre_gt_\n\n_lt_p_gt_\n\tThe higher the gradient_co_ the closer to endValue_co_ right?  The process before was_dd_\n_lt_/p_gt_\n\n_lt_pre_gt_\n_lt_code_gt_result.m[index] _eq_ startValue.m[index] * gradient + endValue.m[index] * (1.0 - gradient)_sm__lt_/code_gt__lt_/pre_gt_\n\n_lt_p_gt_\n\tThe title of the last PR was fixing Lerp.  I really thought this was a great improvement.  I could never get the bone interpolater I am working on with the now DecomposeLerp.  The new one is also much faster. \n_lt_/p_gt_\n\n_lt_p_gt_\n\tBut_co_ if 10 was a start value_co_ 20 the end_co_ &amp_sm_ .2 the gradient_co_ then the answer should be_dd_\n_lt_/p_gt_\n\n_lt_p_gt_\n\t10 + ((20 - 10) * .2) or 12_co_ _lt_strong_gt_not_lt_/strong_gt_\n_lt_/p_gt_\n\n_lt_p_gt_\n\t(10 *.2) + (20 * (1 - .2)) or 18\n_lt_/p_gt_\n\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"},{"Owner":"Deltakosh","Date":"2016-05-13T19:05:00Z","Content":"_lt_div class_eq__qt_mages_qt__gt_\n\t\t\t_lt_p_gt_\n\tOuch...correct. I_t_ll fix it\n_lt_/p_gt_\n\n\t\t\t\n\t\t_lt_/div_gt_\n\n\t\t_lt_div class_eq__qt_ipsI_qt__gt__lt_/div_gt__lt_/div_gt_"}]